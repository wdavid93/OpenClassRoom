{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I) Initialisation of the environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "########## Libraries ##############\n",
    "###################################\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "\n",
    "\n",
    "# The two lines below inform tensorflows that No Cuda cores are present in this machine\n",
    "# it prevents tons of warning messages\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']=\"2\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras import Model\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType, element_at, split\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.ml.functions import array_to_vector\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import PCA, StandardScaler\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Initialisation of Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 13:54:18 WARN Utils: Your hostname, clement-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "23/02/20 13:54:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 13:54:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>P8</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa7333b81f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##############################################\n",
    "##### Initialisation of Spark session ########\n",
    "##############################################\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName('P8')\n",
    "         .master('local')\n",
    "         .config(\"spark.sql.parquet.writeLegacyFormat\", 'true')\n",
    "         .getOrCreate()\n",
    "         )\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"WARN\")\n",
    "spark\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Establishment of the data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PATH_Data:   /home/clement/Documents/P8_data/fruits/fruits-360_dataset/fruits-360/Training\n",
      "PATH_Result: /home/clement/Documents/P8_data/test_local/Results\n",
      "PATH_Pipeline:   /home/clement/Documents/P8_data/test_local/pipeline_trained\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "##### PATH ########################\n",
    "###################################\n",
    "\n",
    "\n",
    "PATH_Data = \"/home/clement/Documents/P8_data/fruits/fruits-360_dataset/fruits-360/Training\"\n",
    "PATH_Result = \"/home/clement/Documents/P8_data/test_local/Results\"\n",
    "PATH_Pipeline = \"/home/clement/Documents/P8_data/test_local/pipeline_trained\"\n",
    "\n",
    "print('\\nPATH_Data:   ' +PATH_Data+\n",
    "'\\nPATH_Result: '+PATH_Result+\n",
    "'\\nPATH_Pipeline:   ' +PATH_Pipeline)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Initialisation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###########################################\n",
    "########## Initialisation of the model ####\n",
    "###########################################\n",
    "model = MobileNetV2(weights='imagenet',\n",
    "                        include_top=True,\n",
    "                        input_shape=(224, 224, 3))\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "new_model = Model(inputs=model.input,\n",
    "                    outputs=model.layers[-2].output)\n",
    "brodcast_weights = sc.broadcast(new_model.get_weights())\n",
    "new_model.set_weights(brodcast_weights.value)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Functions to load, preprocess and generate feature from images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clement/anaconda3/lib/python3.9/site-packages/pyspark/sql/pandas/functions.py:394: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###################################\n",
    "########## Functions ##############\n",
    "###################################\n",
    "\n",
    "\n",
    "def preprocess(content):\n",
    "    \"\"\"\n",
    "    Preprocesses raw image bytes for prediction.\n",
    "    \"\"\"\n",
    "    img = Image.open(io.BytesIO(content)).resize([224, 224])\n",
    "    arr = img_to_array(img)\n",
    "    return preprocess_input(arr)\n",
    "\n",
    "\n",
    "def featurize_series(model, content_series):\n",
    "    \"\"\"\n",
    "    Featurize a pd.Series of raw images using the input model.\n",
    "    :return: a pd.Series of image features\n",
    "    \"\"\"\n",
    "    input = np.stack(content_series.map(preprocess))\n",
    "    \n",
    "    preds = model.predict(input)\n",
    "    # For some layers, output features will be multi-dimensional tensors.\n",
    "    # We flatten the feature tensors to vectors for easier storage in Spark DataFrames.\n",
    "    output = [p.flatten() for p in preds]\n",
    "    return pd.Series(output)\n",
    "\n",
    "\n",
    "def model_fn():\n",
    "    \"\"\"\n",
    "    Returns a MobileNetV2 model with top layer removed \n",
    "    and broadcasted pretrained weights.\n",
    "    \"\"\"\n",
    "    model = MobileNetV2(weights='imagenet',\n",
    "                        include_top=True,\n",
    "                        input_shape=(224, 224, 3))\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    new_model = Model(inputs=model.input,\n",
    "                      outputs=model.layers[-2].output)\n",
    "    new_model.set_weights(brodcast_weights.value)\n",
    "    return new_model\n",
    "\n",
    "\n",
    "@pandas_udf('array<float>', PandasUDFType.SCALAR_ITER)\n",
    "def featurize_udf(content_series_iter):\n",
    "    '''\n",
    "    This method is a Scalar Iterator pandas UDF wrapping our featurization function.\n",
    "    The decorator specifies that this returns a Spark DataFrame column of type ArrayType(FloatType).\n",
    "\n",
    "    :param content_series_iter: This argument is an iterator over batches of data, where each batch\n",
    "                              is a pandas Series of image data.\n",
    "    '''\n",
    "    # With Scalar Iterator pandas UDFs, we can load the model once and then re-use it\n",
    "    # for multiple data batches.  This amortizes the overhead of loading big models.\n",
    "    model = model_fn()\n",
    "    for content_series in content_series_iter:\n",
    "        yield featurize_series(model, content_series)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II) Feature extraction and PCA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Loading image, preprocessing and feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- modificationTime: timestamp (nullable = true)\n",
      " |-- length: long (nullable = true)\n",
      " |-- content: binary (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n",
      "None\n",
      "+-------------------------------------------------------------------------------------------------------------+--------------+\n",
      "|path                                                                                                         |label         |\n",
      "+-------------------------------------------------------------------------------------------------------------+--------------+\n",
      "|file:/home/clement/Documents/P8_data/fruits/fruits-360_dataset/fruits-360/Training/Raspberry/176_100.jpg     |Raspberry     |\n",
      "|file:/home/clement/Documents/P8_data/fruits/fruits-360_dataset/fruits-360/Training/Raspberry/179_100.jpg     |Raspberry     |\n",
      "|file:/home/clement/Documents/P8_data/fruits/fruits-360_dataset/fruits-360/Training/Pineapple Mini/170_100.jpg|Pineapple Mini|\n",
      "|file:/home/clement/Documents/P8_data/fruits/fruits-360_dataset/fruits-360/Training/Raspberry/157_100.jpg     |Raspberry     |\n",
      "|file:/home/clement/Documents/P8_data/fruits/fruits-360_dataset/fruits-360/Training/Raspberry/131_100.jpg     |Raspberry     |\n",
      "+-------------------------------------------------------------------------------------------------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n",
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "##### Loading images ##############\n",
    "###################################\n",
    "images = spark.read.format(\"binaryFile\") \\\n",
    "    .option(\"pathGlobFilter\", \"*.jpg\") \\\n",
    "    .option(\"recursiveFileLookup\", \"true\") \\\n",
    "    .load(PATH_Data)\n",
    "\n",
    "\n",
    "images = images.withColumn('label', element_at(\n",
    "    split(images['path'], '/'), -2)) \n",
    "print(images.printSchema())\n",
    "print(images.select('path', 'label').show(5, False))\n",
    "\n",
    "\n",
    "features_df = images.repartition(20).select(col(\"path\"), col(\n",
    "    \"label\"), featurize_udf(\"content\").alias(\"features\")).withColumn(\"features\", array_to_vector(\"features\")).cache()\n",
    "\n",
    "# Array_to_vector is necessary before scaling and PCA\n",
    "# Optimisation : Caching data is necessary to prevent computing the data 2 times (in the scaling + in the PCA)\n",
    "\n",
    "\n",
    "\n",
    "print(features_df.printSchema())\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) PCA : Creation and fitting a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 65s 610ms/step\n",
      "105/105 [==============================] - 69s 641ms/step\n",
      "106/106 [==============================] - 68s 639ms/step\n",
      "106/106 [==============================] - 66s 615ms/step\n",
      "108/108 [==============================] - 68s 625ms/step\n",
      "108/108 [==============================] - 67s 614ms/step\n",
      "108/108 [==============================] - 64s 585ms/step\n",
      "108/108 [==============================] - 67s 616ms/step\n",
      "108/108 [==============================] - 67s 615ms/step\n",
      "107/107 [==============================] - 66s 607ms/step\n",
      "107/107 [==============================] - 65s 602ms/step\n",
      "107/107 [==============================] - 65s 595ms/step\n",
      "106/106 [==============================] - 63s 584ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/17 11:12:09 WARN MemoryStore: Not enough space to cache rdd_31_12 in memory! (computed 34.2 MiB so far)\n",
      "23/02/17 11:12:09 WARN BlockManager: Persisting block rdd_31_12 to disk instead.\n",
      "23/02/17 11:12:09 WARN MemoryStore: Not enough space to cache rdd_31_12 in memory! (computed 34.2 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 51s 471ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/17 11:13:08 WARN MemoryStore: Not enough space to cache rdd_31_13 in memory! (computed 34.0 MiB so far)\n",
      "23/02/17 11:13:08 WARN BlockManager: Persisting block rdd_31_13 to disk instead.\n",
      "23/02/17 11:13:08 WARN MemoryStore: Not enough space to cache rdd_31_13 in memory! (computed 34.0 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 50s 464ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/17 11:14:06 WARN MemoryStore: Not enough space to cache rdd_31_14 in memory! (computed 34.0 MiB so far)\n",
      "23/02/17 11:14:06 WARN BlockManager: Persisting block rdd_31_14 to disk instead.\n",
      "23/02/17 11:14:07 WARN MemoryStore: Not enough space to cache rdd_31_14 in memory! (computed 34.0 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 57s 533ms/step        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/17 11:15:11 WARN MemoryStore: Not enough space to cache rdd_31_15 in memory! (computed 33.7 MiB so far)\n",
      "23/02/17 11:15:11 WARN BlockManager: Persisting block rdd_31_15 to disk instead.\n",
      "23/02/17 11:15:12 WARN MemoryStore: Not enough space to cache rdd_31_15 in memory! (computed 33.7 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 55s 517ms/step         (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/17 11:16:16 WARN MemoryStore: Not enough space to cache rdd_31_16 in memory! (computed 33.6 MiB so far)\n",
      "23/02/17 11:16:16 WARN BlockManager: Persisting block rdd_31_16 to disk instead.\n",
      "23/02/17 11:16:17 WARN MemoryStore: Not enough space to cache rdd_31_16 in memory! (computed 33.6 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 56s 527ms/step>       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/17 11:17:21 WARN MemoryStore: Not enough space to cache rdd_31_17 in memory! (computed 33.6 MiB so far)\n",
      "23/02/17 11:17:21 WARN BlockManager: Persisting block rdd_31_17 to disk instead.\n",
      "23/02/17 11:17:22 WARN MemoryStore: Not enough space to cache rdd_31_17 in memory! (computed 33.6 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 52s 490ms/step===>     (18 + 1) / 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/17 11:18:23 WARN MemoryStore: Not enough space to cache rdd_31_18 in memory! (computed 33.8 MiB so far)\n",
      "23/02/17 11:18:23 WARN BlockManager: Persisting block rdd_31_18 to disk instead.\n",
      "23/02/17 11:18:23 WARN MemoryStore: Not enough space to cache rdd_31_18 in memory! (computed 33.8 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 55s 511ms/step======>  (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/17 11:19:27 WARN MemoryStore: Not enough space to cache rdd_31_19 in memory! (computed 33.8 MiB so far)\n",
      "23/02/17 11:19:27 WARN BlockManager: Persisting block rdd_31_19 to disk instead.\n",
      "23/02/17 11:19:27 WARN MemoryStore: Not enough space to cache rdd_31_19 in memory! (computed 33.8 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:=================================>                      (12 + 1) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/17 11:19:37 WARN MemoryStore: Not enough space to cache rdd_31_12 in memory! (computed 34.2 MiB so far)\n",
      "23/02/17 11:19:37 WARN MemoryStore: Not enough space to cache rdd_31_13 in memory! (computed 34.0 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:====================================>                   (13 + 1) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/17 11:19:38 WARN MemoryStore: Not enough space to cache rdd_31_14 in memory! (computed 34.0 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:=======================================>                (14 + 1) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/17 11:19:38 WARN MemoryStore: Not enough space to cache rdd_31_15 in memory! (computed 33.7 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:==========================================>             (15 + 1) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/17 11:19:38 WARN MemoryStore: Not enough space to cache rdd_31_16 in memory! (computed 33.6 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:============================================>           (16 + 1) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/17 11:19:39 WARN MemoryStore: Not enough space to cache rdd_31_17 in memory! (computed 33.6 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:===============================================>        (17 + 1) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/17 11:19:39 WARN MemoryStore: Not enough space to cache rdd_31_18 in memory! (computed 33.8 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:=====================================================>  (19 + 1) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/17 11:19:39 WARN MemoryStore: Not enough space to cache rdd_31_19 in memory! (computed 33.8 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:==============================>                         (11 + 1) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/17 11:20:16 WARN MemoryStore: Not enough space to cache rdd_31_12 in memory! (computed 34.2 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:====================================>                   (13 + 1) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/17 11:20:20 WARN MemoryStore: Not enough space to cache rdd_31_13 in memory! (computed 34.0 MiB so far)\n",
      "23/02/17 11:20:23 WARN MemoryStore: Not enough space to cache rdd_31_14 in memory! (computed 34.0 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:=======================================>                (14 + 1) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/17 11:20:26 WARN MemoryStore: Not enough space to cache rdd_31_15 in memory! (computed 33.7 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:==========================================>             (15 + 1) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/17 11:20:29 WARN MemoryStore: Not enough space to cache rdd_31_16 in memory! (computed 33.6 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:===============================================>        (17 + 1) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/17 11:20:33 WARN MemoryStore: Not enough space to cache rdd_31_17 in memory! (computed 33.6 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:==================================================>     (18 + 1) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/17 11:20:36 WARN MemoryStore: Not enough space to cache rdd_31_18 in memory! (computed 33.8 MiB so far)\n",
      "23/02/17 11:20:39 WARN MemoryStore: Not enough space to cache rdd_31_19 in memory! (computed 33.8 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/17 11:20:43 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n",
      "23/02/17 11:20:43 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n",
      "0.7980893660716227\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "##### Standard Scaling + PCA ######\n",
    "###################################\n",
    "\n",
    "# Initialisation of the two stages of the pipeline\n",
    "Scalerizer = StandardScaler(inputCol=\"features\", outputCol=\"Scaled_features\")\n",
    "PCA_model = PCA(k=200,inputCol=\"Scaled_features\", outputCol=\"pca_features\")\n",
    "\n",
    "\n",
    "# Creation of the pipeline\n",
    "pipeline = Pipeline(stages=[Scalerizer, PCA_model])\n",
    "\n",
    "\n",
    "# fitting of the pipeline\n",
    "pipeline_model = pipeline.fit(features_df)\n",
    "\n",
    "# Explained variance of the 200 first components\n",
    "print(\"Variance explained by the 200 first components: \",pipeline_model.stages[1].explainedVariance.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "##### Applying of the pipeline : transform data ####\n",
    "####################################################\n",
    "\n",
    "features_df = pipeline_model.transform(features_df)\n",
    "\n",
    "features_df.show(5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Exporting the data and the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "##### Export output ###############\n",
    "###################################\n",
    "\n",
    "# Export the dataframe (parquet format)\n",
    "features_df.write.mode(\"overwrite\").parquet(PATH_Result)\n",
    "\n",
    "\n",
    "# Export the fitted pipeline (scaling+PCA)\n",
    "pipeline_model.write().overwrite().save(PATH_Pipeline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15dd4a854850eefb5ef4f8c3a668de1959880b0efc885fe0295a6a7006c343e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
