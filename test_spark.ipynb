{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUtjmo7LctGgNpTYvvjykE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wdavid93/OpenClassRoom/blob/main/test_spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoEE0LOHRa9c",
        "outputId": "5fa4c006-f3f8-46e4-d726-5b33ff6b91a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  spark-3.1.1-bin-hadoop3.2\tspark-3.1.1-bin-hadoop3.2.tgz\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "oXx4kXcPRkkx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "K3IWRpq-lXD3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://s3.eu-west-1.amazonaws.com/course.oc-static.com/projects/Data_Scientist_P8/fruits.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEMz8TFLnF9Z",
        "outputId": "ad1dc83f-07a5-4b6a-bba1-933daf29ce4b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-06 14:17:40--  https://s3.eu-west-1.amazonaws.com/course.oc-static.com/projects/Data_Scientist_P8/fruits.zip\n",
            "Resolving s3.eu-west-1.amazonaws.com (s3.eu-west-1.amazonaws.com)... 52.218.118.0, 52.92.36.152, 52.218.41.235, ...\n",
            "Connecting to s3.eu-west-1.amazonaws.com (s3.eu-west-1.amazonaws.com)|52.218.118.0|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1379678841 (1.3G) [application/zip]\n",
            "Saving to: ‘fruits.zip’\n",
            "\n",
            "fruits.zip          100%[===================>]   1.28G  16.6MB/s    in 82s     \n",
            "\n",
            "2023-12-06 14:19:03 (16.0 MB/s) - ‘fruits.zip’ saved [1379678841/1379678841]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q fruits.zip"
      ],
      "metadata": {
        "id": "hU4xnrdYnKHh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1V8P3j7kgpwu",
        "outputId": "d07d94bb-9f3c-4f5a-8a45-779f68cd82df"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fruits-360_dataset\t  fruits.zip   spark-3.1.1-bin-hadoop3.2\n",
            "fruits-360-original-size  sample_data  spark-3.1.1-bin-hadoop3.2.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.ml.image import ImageSchema\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "# Charger les images\n",
        "images_df = spark.read.format(\"image\").load(\"fruits-360_dataset/fruits-360/Test/Apple Braeburn\")"
      ],
      "metadata": {
        "id": "bimstDAMnn5i"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-4GQHzVn27L",
        "outputId": "5fc7ab02-303d-411b-daef-c4a63ae57f40"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- image: struct (nullable = true)\n",
            " |    |-- origin: string (nullable = true)\n",
            " |    |-- height: integer (nullable = true)\n",
            " |    |-- width: integer (nullable = true)\n",
            " |    |-- nChannels: integer (nullable = true)\n",
            " |    |-- mode: integer (nullable = true)\n",
            " |    |-- data: binary (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images_df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWviwIr_oDpX",
        "outputId": "e9d428d5-f7c4-4ba9-d9e6-388c43027786"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "164"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import os\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "# Chemin d'accès au répertoire des images\n",
        "image_dir = \"fruits-360_dataset/fruits-360/Test/Apple Braeburn\"\n",
        "\n",
        "# Vérifier si le répertoire existe\n",
        "if os.path.exists(image_dir):\n",
        "    # Charger les images\n",
        "    images_df = spark.read.format(\"image\").load(image_dir)\n",
        "    # Afficher les premières lignes de images_df\n",
        "    # images_df.show()\n",
        "    print(f\"Le répertoire {image_dir} existe.\")\n",
        "    # images_df.show()\n",
        "else:\n",
        "    print(f\"Le répertoire {image_dir} n'existe pas.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AH2FCIsAoLvH",
        "outputId": "56a8baed-2d3a-4d3c-d0da-d4c00a18dfad"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Le répertoire fruits-360_dataset/fruits-360/Test/Apple Braeburn existe.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l 'fruits-360_dataset/fruits-360/Test/Apple Braeburn/r_4_100.jpg'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvUfz7AqqxJG",
        "outputId": "e1d3d39c-2b55-4e3a-b2f0-f55f2c929025"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 5473 Sep 12  2021 'fruits-360_dataset/fruits-360/Test/Apple Braeburn/r_4_100.jpg'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import pyspark\n",
        "import time\n",
        "from pyspark import SQLContext\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.image import ImageSchema\n",
        "\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.functions import input_file_name\n",
        "from pyspark.sql.types import *\n",
        "#import pyspark\n",
        "\n",
        "def parse_categorie(path):\n",
        "    '''Renvoie la catégorie d\\'une image à partir de son chemin'''\n",
        "    if len(path) > 0:\n",
        "        #catégorie de l'image\n",
        "        return path.split('/')[-2]\n",
        "    else:\n",
        "        return ''\n",
        "\n",
        "def import_dossier(path):\n",
        "    '''Renvoie un dataframe spark des images à partir du chemin du dossier.\n",
        "    :param path: chemin vers le dossier\n",
        "    :return: dataframe spark contenant toutes les images du dossier'''\n",
        "\n",
        "\n",
        "def load_data(path_img):\n",
        "    '''Chargement des dataframes:\n",
        "    Prend en entrée le répertoire qui contient les sous répertoires contenant les images\n",
        "    Renvoie en sortie un spark dataframe contenant les images et\n",
        "    un spark dataframe contenant les noms des fruits associés'''\n",
        "    #compteur\n",
        "    start = time.time()\n",
        "    #chargement dataframe des images\n",
        "\n",
        "    df_img = spark.read.format(\"image\").load(path) # ne fonctionne pas si il y a des espaces dans le chemin\n",
        "    #df_img =  ImageSchema.readImages(path_img, dropImageFailures = True)\n",
        "    print('chargement effectué')\n",
        "    #récupération chemin à partir des images\n",
        "    df_img = df_img.withColumn(\"path\", input_file_name())\n",
        "    #catégorisation des images\n",
        "    udf_categorie = udf(parse_categorie, StringType())\n",
        "    df_img = df_img.withColumn('categorie', udf_categorie('path'))\n",
        "    print('Temps de chargement des images : {} secondes'.format(time.strftime('%S', time.gmtime(time.time()-start))))\n",
        "\n",
        "    return df_img"
      ],
      "metadata": {
        "id": "l_Tfh0__vbUk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#chargement du dataframe contenant les images, leur chemin et leur catégorie\n",
        "path = \"fruits-360_dataset/fruits-360/Test/Apple Braeburn\"\n",
        "spark_df = load_data(path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NSeefPJvnIw",
        "outputId": "b604ccc2-6f15-4a32-e43b-fde3585cc042"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chargement effectué\n",
            "Temps de chargement des images : 00 secondes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark_df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swWAbezlv2CY",
        "outputId": "ea9cf372-f3fc-432f-d7b5-b2c7187b7dd7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "164"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import ArrayType, FloatType\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Charger le modèle ResNet50 pré-entraîné sans la dernière couche de classification\n",
        "# base_model = ResNet50(weights='imagenet', include_top=False)\n",
        "# model = Model(inputs=base_model.input, outputs=base_model.get_layer('avg_pool').output)\n",
        "# Chargement du modèle ResNet50 sans poids\n",
        "model = ResNet50(weights=None)\n",
        "\n",
        "# Fonction pour charger et prétraiter l'image\n",
        "def preprocess_image(image_path):\n",
        "    # Charger l'image\n",
        "    img = load_img(image_path, target_size=(224, 224))\n",
        "    # Convertir en tableau numpy\n",
        "    img_array = img_to_array(img)\n",
        "    # Ajouter une dimension pour créer un batch de taille 1\n",
        "    img_batch = np.expand_dims(img_array, axis=0)\n",
        "    # Prétraiter l'image pour le modèle ResNet50\n",
        "    img_preprocessed = preprocess_input(img_batch)\n",
        "    return img_preprocessed\n",
        "\n",
        "# Fonction UDF pour appliquer le modèle à l'image et obtenir les caractéristiques\n",
        "def extract_features(image_path):\n",
        "    img_preprocessed = preprocess_image(image_path)\n",
        "    features = model.predict(img_preprocessed)\n",
        "    # Aplatir les caractéristiques en une liste pour les stocker dans une colonne DataFrame\n",
        "    return features.flatten().tolist()\n",
        "\n",
        "# Enregistrer la fonction UDF avec le type de retour approprié\n",
        "extract_features_udf = udf(extract_features, ArrayType(FloatType()))\n",
        "\n",
        "def preprocess_data(dataframe):\n",
        "    '''Renvoie le résultat de l'avant dernière couche de chaque image du dataframe via le modèle ResNet50\n",
        "    return un df contenant des vecteurs de dimension 1x2048 '''\n",
        "\n",
        "    # Appliquer la fonction UDF pour extraire les caractéristiques\n",
        "    output = dataframe.withColumn('image_preprocessed', extract_features_udf(col('path')))\n",
        "    # Sélectionner les colonnes nécessaires\n",
        "    output = output.select(['path', 'categorie', 'image_preprocessed'])\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "Y9JNlg7AwAMz"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark_df_preprocessed = preprocess_data(spark_df)"
      ],
      "metadata": {
        "id": "Zj7LWp_RwHOc"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark_df_preprocessed.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SBxKvgZwLFC",
        "outputId": "a6a03c93-935f-4470-8080-a77846220d9d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "164"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "schema = spark_df_preprocessed.schema\n",
        "schema"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPBAhMsthLIQ",
        "outputId": "bce1d6c8-f1a7-4368-b5d4-75a181db18b2"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StructType(List(StructField(path,StringType,false),StructField(categorie,StringType,true),StructField(image_preprocessed,ArrayType(FloatType,true),true)))"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sélectionner la colonne 'path' et afficher une seule ligne\n",
        "spark_df_preprocessed.select('path').show(1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsTsIO7fh5sZ",
        "outputId": "1dbbedf8-83e5-4648-ef80-f9cb768b980b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|                path|\n",
            "+--------------------+\n",
            "|file:///content/f...|\n",
            "+--------------------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sélectionner la colonne 'categorie' et afficher une seule ligne\n",
        "spark_df_preprocessed.select('categorie').show(1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3P6kSChiVY7",
        "outputId": "7cd49d91-c55c-46ed-e0c9-c6804f26a13b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+\n",
            "|       categorie|\n",
            "+----------------+\n",
            "|Apple%20Braeburn|\n",
            "+----------------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sélectionner la colonne 'image_preprocessed' et afficher une seule ligne\n",
        "# spark_df_preprocessed.select('image_preprocessed').show(10)"
      ],
      "metadata": {
        "id": "j1IKg85yjX6_"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "sc = SparkContext.getOrCreate()"
      ],
      "metadata": {
        "id": "LSyy54AOl5MM"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Return a JavaRDD of Object by unpickling\n",
        "It will convert each Python object into Java object by Pyrolite, whenever the\n",
        "RDD is serialized in batch or not.\n",
        "\"\"\"\n",
        "\n",
        "from pyspark.serializers import PickleSerializer, AutoBatchedSerializer\n",
        "\n",
        "# Function to convert python object to Java objects\n",
        "def _to_java_object_rdd(rdd):\n",
        "    rdd = rdd._reserialize(AutoBatchedSerializer(PickleSerializer()))\n",
        "    return rdd.ctx._jvm.org.apache.spark.mllib.api.python.SerDe.pythonToJava(rdd._jrdd, True)\n",
        "\n",
        "# Convert DataFrame to an RDD\n",
        "JavaObj = _to_java_object_rdd(images_df.rdd)\n",
        "\n",
        "# Estimate size in bytes\n",
        "nbytes = sc._jvm.org.apache.spark.util.SizeEstimator.estimate(JavaObj)\n",
        "nbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9oNqiewlf4s",
        "outputId": "c12e65f3-cbb1-4f8e-e473-fd362e3f874f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "118701192"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o57wRxSon3Ae",
        "outputId": "db88ab15-c3e3-4c96-8447-f1e2749a64ef"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<py4j.java_gateway.JavaMember at 0x7feb3fd8dba0>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "schema = spark_df_preprocessed.schema\n",
        "schema\n",
        "# # Supposons que JavaObj est votre RDD Java\n",
        "# # Convertir le RDD Java en DataFrame\n",
        "# df = spark.createDataFrame(JavaObj, schema)\n",
        "\n",
        "# # Afficher le DataFrame\n",
        "# df.show()\n",
        "\n",
        "\n",
        "# # Supposons que JavaObj est votre RDD Java\n",
        "# # Convertir le RDD Java en RDD de tuples\n",
        "# rdd = JavaObj.map(lambda x: tuple(x))\n",
        "\n",
        "# # Créer le DataFrame à partir du RDD de tuples et du schéma\n",
        "# df = spark.createDataFrame(rdd, schema)\n",
        "\n",
        "# # Afficher le DataFrame\n",
        "# df.show()\n",
        "\n",
        "# # Supposons que JavaObj est votre objet Java\n",
        "# java_object_list = JavaObj.get_data()  # Utilisez la méthode appropriée pour extraire les données\n",
        "\n",
        "# # Convertir la liste en RDD\n",
        "# rdd_from_java = sc.parallelize(java_object_list)\n",
        "\n",
        "# # Convertir le RDD en DataFrame en utilisant le schéma\n",
        "# df = spark.createDataFrame(rdd_from_java, schema)\n",
        "\n",
        "# # Afficher le DataFrame\n",
        "# df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaVAosfTmB8N",
        "outputId": "a1c6695b-ac30-4535-9a6e-375326100d24"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StructType(List(StructField(path,StringType,false),StructField(categorie,StringType,true),StructField(image_preprocessed,ArrayType(FloatType,true),true)))"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import PCA\n",
        "\n",
        "# instantiate Spark PCA model\n",
        "pca = PCA(k=8,\n",
        "          inputCol=\"image_preprocessed\",\n",
        "          outputCol=\"pca_features\")"
      ],
      "metadata": {
        "id": "1rJ9PVW-o0SO"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.sql.functions import udf\n",
        "# from pyspark.sql.types import VectorUDT\n",
        "\n",
        "# # Définir une fonction UDF pour convertir array<float> en VectorUDT\n",
        "# to_vector_udf = udf(lambda arr: Vectors.dense(arr), VectorUDT())\n",
        "\n",
        "# # Appliquer la fonction UDF pour créer une nouvelle colonne de type VectorUDT\n",
        "# spark_df_preprocessed = spark_df_preprocessed.withColumn(\"image_preprocessed_vector\", to_vector_udf(\"image_preprocessed\"))\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import ArrayType, FloatType\n",
        "\n",
        "# Définir une fonction UDF pour convertir array<float> en vecteur dense\n",
        "to_vector_udf = udf(lambda arr: Vectors.dense(arr), ArrayType(FloatType()))\n",
        "\n",
        "# Appliquer la fonction UDF pour créer une nouvelle colonne de type vecteur dense\n",
        "spark_df_preprocessed = spark_df_preprocessed.withColumn(\"image_preprocessed_vector\", to_vector_udf(\"image_preprocessed\"))\n"
      ],
      "metadata": {
        "id": "0Xd07nVtr_bd"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark_df_preprocessed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLyw8bH8y8p_",
        "outputId": "b1361a0c-6cb2-4fb6-fdb6-9032edb99fac"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[path: string, categorie: string, image_preprocessed: array<float>, image_preprocessed_vector: array<float>]"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark_df_preprocessed.select('image_preprocessed_vector')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9R7iCzdxpL5P",
        "outputId": "3a325196-634d-4211-fc31-a373cbdca416"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[image_preprocessed_vector: array<float>]"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fit the model on the extracted features\n",
        "model = pca.fit(spark_df_preprocessed.select('image_preprocessed_vector'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "v_xGiWXEzidy",
        "outputId": "5e40c042-0959-42c4-9380-0b60dec4743f"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IllegalArgumentException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-9ffb03ce081b>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# fit the model on the extracted features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_df_preprocessed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'image_preprocessed_vector'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \"\"\"\n\u001b[1;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: image_preprocessed does not exist. Available: image_preprocessed_vector"
          ]
        }
      ]
    }
  ]
}