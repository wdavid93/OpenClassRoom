{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPPpKaoNm1oXZXeU3vuwdA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wdavid93/OpenClassRoom/blob/main/test_spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoEE0LOHRa9c",
        "outputId": "5fa4c006-f3f8-46e4-d726-5b33ff6b91a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  spark-3.1.1-bin-hadoop3.2\tspark-3.1.1-bin-hadoop3.2.tgz\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "oXx4kXcPRkkx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "K3IWRpq-lXD3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://s3.eu-west-1.amazonaws.com/course.oc-static.com/projects/Data_Scientist_P8/fruits.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEMz8TFLnF9Z",
        "outputId": "ad1dc83f-07a5-4b6a-bba1-933daf29ce4b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-06 14:17:40--  https://s3.eu-west-1.amazonaws.com/course.oc-static.com/projects/Data_Scientist_P8/fruits.zip\n",
            "Resolving s3.eu-west-1.amazonaws.com (s3.eu-west-1.amazonaws.com)... 52.218.118.0, 52.92.36.152, 52.218.41.235, ...\n",
            "Connecting to s3.eu-west-1.amazonaws.com (s3.eu-west-1.amazonaws.com)|52.218.118.0|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1379678841 (1.3G) [application/zip]\n",
            "Saving to: ‘fruits.zip’\n",
            "\n",
            "fruits.zip          100%[===================>]   1.28G  16.6MB/s    in 82s     \n",
            "\n",
            "2023-12-06 14:19:03 (16.0 MB/s) - ‘fruits.zip’ saved [1379678841/1379678841]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q fruits.zip"
      ],
      "metadata": {
        "id": "hU4xnrdYnKHh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1V8P3j7kgpwu",
        "outputId": "d07d94bb-9f3c-4f5a-8a45-779f68cd82df"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fruits-360_dataset\t  fruits.zip   spark-3.1.1-bin-hadoop3.2\n",
            "fruits-360-original-size  sample_data  spark-3.1.1-bin-hadoop3.2.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.ml.image import ImageSchema\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "# Charger les images\n",
        "images_df = spark.read.format(\"image\").load(\"fruits-360_dataset/fruits-360/Test/Apple Braeburn\")"
      ],
      "metadata": {
        "id": "bimstDAMnn5i"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-4GQHzVn27L",
        "outputId": "5fc7ab02-303d-411b-daef-c4a63ae57f40"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- image: struct (nullable = true)\n",
            " |    |-- origin: string (nullable = true)\n",
            " |    |-- height: integer (nullable = true)\n",
            " |    |-- width: integer (nullable = true)\n",
            " |    |-- nChannels: integer (nullable = true)\n",
            " |    |-- mode: integer (nullable = true)\n",
            " |    |-- data: binary (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images_df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWviwIr_oDpX",
        "outputId": "e9d428d5-f7c4-4ba9-d9e6-388c43027786"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "164"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import os\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "# Chemin d'accès au répertoire des images\n",
        "image_dir = \"fruits-360_dataset/fruits-360/Test/Apple Braeburn\"\n",
        "\n",
        "# Vérifier si le répertoire existe\n",
        "if os.path.exists(image_dir):\n",
        "    # Charger les images\n",
        "    images_df = spark.read.format(\"image\").load(image_dir)\n",
        "    # Afficher les premières lignes de images_df\n",
        "    # images_df.show()\n",
        "    print(f\"Le répertoire {image_dir} existe.\")\n",
        "    # images_df.show()\n",
        "else:\n",
        "    print(f\"Le répertoire {image_dir} n'existe pas.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AH2FCIsAoLvH",
        "outputId": "56a8baed-2d3a-4d3c-d0da-d4c00a18dfad"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Le répertoire fruits-360_dataset/fruits-360/Test/Apple Braeburn existe.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l 'fruits-360_dataset/fruits-360/Test/Apple Braeburn/r_4_100.jpg'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvUfz7AqqxJG",
        "outputId": "e1d3d39c-2b55-4e3a-b2f0-f55f2c929025"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 5473 Sep 12  2021 'fruits-360_dataset/fruits-360/Test/Apple Braeburn/r_4_100.jpg'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import pyspark\n",
        "import time\n",
        "from pyspark import SQLContext\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.image import ImageSchema\n",
        "\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.functions import input_file_name\n",
        "from pyspark.sql.types import *\n",
        "#import pyspark\n",
        "\n",
        "def parse_categorie(path):\n",
        "    '''Renvoie la catégorie d\\'une image à partir de son chemin'''\n",
        "    if len(path) > 0:\n",
        "        #catégorie de l'image\n",
        "        return path.split('/')[-2]\n",
        "    else:\n",
        "        return ''\n",
        "\n",
        "def import_dossier(path):\n",
        "    '''Renvoie un dataframe spark des images à partir du chemin du dossier.\n",
        "    :param path: chemin vers le dossier\n",
        "    :return: dataframe spark contenant toutes les images du dossier'''\n",
        "\n",
        "\n",
        "def load_data(path_img):\n",
        "    '''Chargement des dataframes:\n",
        "    Prend en entrée le répertoire qui contient les sous répertoires contenant les images\n",
        "    Renvoie en sortie un spark dataframe contenant les images et\n",
        "    un spark dataframe contenant les noms des fruits associés'''\n",
        "    #compteur\n",
        "    start = time.time()\n",
        "    #chargement dataframe des images\n",
        "\n",
        "    df_img = spark.read.format(\"image\").load(path) # ne fonctionne pas si il y a des espaces dans le chemin\n",
        "    #df_img =  ImageSchema.readImages(path_img, dropImageFailures = True)\n",
        "    print('chargement effectué')\n",
        "    #récupération chemin à partir des images\n",
        "    df_img = df_img.withColumn(\"path\", input_file_name())\n",
        "    #catégorisation des images\n",
        "    udf_categorie = udf(parse_categorie, StringType())\n",
        "    df_img = df_img.withColumn('categorie', udf_categorie('path'))\n",
        "    print('Temps de chargement des images : {} secondes'.format(time.strftime('%S', time.gmtime(time.time()-start))))\n",
        "\n",
        "    return df_img"
      ],
      "metadata": {
        "id": "l_Tfh0__vbUk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#chargement du dataframe contenant les images, leur chemin et leur catégorie\n",
        "path = \"fruits-360_dataset/fruits-360/Test/Apple Braeburn\"\n",
        "spark_df = load_data(path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NSeefPJvnIw",
        "outputId": "b604ccc2-6f15-4a32-e43b-fde3585cc042"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chargement effectué\n",
            "Temps de chargement des images : 00 secondes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark_df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swWAbezlv2CY",
        "outputId": "ea9cf372-f3fc-432f-d7b5-b2c7187b7dd7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "164"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import ArrayType, FloatType\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Charger le modèle ResNet50 pré-entraîné sans la dernière couche de classification\n",
        "# base_model = ResNet50(weights='imagenet', include_top=False)\n",
        "# model = Model(inputs=base_model.input, outputs=base_model.get_layer('avg_pool').output)\n",
        "# Chargement du modèle ResNet50 sans poids\n",
        "model = ResNet50(weights=None)\n",
        "\n",
        "# Fonction pour charger et prétraiter l'image\n",
        "def preprocess_image(image_path):\n",
        "    # Charger l'image\n",
        "    img = load_img(image_path, target_size=(224, 224))\n",
        "    # Convertir en tableau numpy\n",
        "    img_array = img_to_array(img)\n",
        "    # Ajouter une dimension pour créer un batch de taille 1\n",
        "    img_batch = np.expand_dims(img_array, axis=0)\n",
        "    # Prétraiter l'image pour le modèle ResNet50\n",
        "    img_preprocessed = preprocess_input(img_batch)\n",
        "    return img_preprocessed\n",
        "\n",
        "# Fonction UDF pour appliquer le modèle à l'image et obtenir les caractéristiques\n",
        "def extract_features(image_path):\n",
        "    img_preprocessed = preprocess_image(image_path)\n",
        "    features = model.predict(img_preprocessed)\n",
        "    # Aplatir les caractéristiques en une liste pour les stocker dans une colonne DataFrame\n",
        "    return features.flatten().tolist()\n",
        "\n",
        "# Enregistrer la fonction UDF avec le type de retour approprié\n",
        "extract_features_udf = udf(extract_features, ArrayType(FloatType()))\n",
        "\n",
        "def preprocess_data(dataframe):\n",
        "    '''Renvoie le résultat de l'avant dernière couche de chaque image du dataframe via le modèle ResNet50\n",
        "    return un df contenant des vecteurs de dimension 1x2048 '''\n",
        "\n",
        "    # Appliquer la fonction UDF pour extraire les caractéristiques\n",
        "    output = dataframe.withColumn('image_preprocessed', extract_features_udf(col('path')))\n",
        "    # Sélectionner les colonnes nécessaires\n",
        "    output = output.select(['path', 'categorie', 'image_preprocessed'])\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "Y9JNlg7AwAMz"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark_df_preprocessed = preprocess_data(spark_df)"
      ],
      "metadata": {
        "id": "Zj7LWp_RwHOc"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark_df_preprocessed.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SBxKvgZwLFC",
        "outputId": "a6a03c93-935f-4470-8080-a77846220d9d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "164"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark_df_preprocessed.schema"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPBAhMsthLIQ",
        "outputId": "561b9c90-13b3-4f3d-a780-11c7989bdf13"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StructType(List(StructField(path,StringType,false),StructField(categorie,StringType,true),StructField(image_preprocessed,ArrayType(FloatType,true),true)))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sélectionner la colonne 'path' et afficher une seule ligne\n",
        "spark_df_preprocessed.select('path').show(1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsTsIO7fh5sZ",
        "outputId": "1dbbedf8-83e5-4648-ef80-f9cb768b980b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|                path|\n",
            "+--------------------+\n",
            "|file:///content/f...|\n",
            "+--------------------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sélectionner la colonne 'categorie' et afficher une seule ligne\n",
        "spark_df_preprocessed.select('categorie').show(1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3P6kSChiVY7",
        "outputId": "7cd49d91-c55c-46ed-e0c9-c6804f26a13b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+\n",
            "|       categorie|\n",
            "+----------------+\n",
            "|Apple%20Braeburn|\n",
            "+----------------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sélectionner la colonne 'image_preprocessed' et afficher une seule ligne\n",
        "spark_df_preprocessed.select('image_preprocessed').show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 772
        },
        "id": "j1IKg85yjX6_",
        "outputId": "53434b16-c289-470e-c9df-7e049052c81a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "PythonException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-be4e7f235e93>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Sélectionner la colonne 'image_preprocessed' et afficher une seule ligne\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mspark_df_preprocessed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'image_preprocessed'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \"\"\"\n\u001b[1;32m    483\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/content/spark-3.1.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/content/spark-3.1.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/content/spark-3.1.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 211, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/content/spark-3.1.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 132, in dump_stream\n    for obj in iterator:\n  File \"/content/spark-3.1.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 200, in _batched\n    for item in iterator:\n  File \"/content/spark-3.1.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/content/spark-3.1.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/content/spark-3.1.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n    return lambda *a: f(*a)\n  File \"/content/spark-3.1.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-16-8584edc00691>\", line 29, in extract_features\n  File \"<ipython-input-16-8584edc00691>\", line 19, in preprocess_image\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/image_utils.py\", line 422, in load_img\n    with open(path, \"rb\") as f:\nFileNotFoundError: [Errno 2] No such file or directory: 'file:///content/fruits-360_dataset/fruits-360/Test/Apple%20Braeburn/r_4_100.jpg'\n"
          ]
        }
      ]
    }
  ]
}