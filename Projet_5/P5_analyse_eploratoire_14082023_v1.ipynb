{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consignes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "projet : https://openclassrooms.com/fr/projects/630/assignment\n",
    "données : https://www.kaggle.com/olistbr/brazilian-ecommerce\n",
    "\n",
    "Olist souhaite que vous fournissiez à ses équipes d'e-commerce une segmentation des clients qu’elles pourront utiliser au quotidien pour leurs campagnes de communication.\n",
    "\n",
    "Enfin, votre client, Olist, a spécifié sa demande ainsi :\n",
    "\n",
    "* La segmentation proposée doit être exploitable et facile d’utilisation pour l’équipe marketing.\n",
    "* Vous évaluerez la fréquence à laquelle la segmentation doit être mise à jour, afin de pouvoir effectuer un devis de contrat de maintenance.\n",
    "* Le code fourni doit respecter la convention PEP8, pour être utilisable par Olist.\n",
    "\n",
    "Livrables\n",
    "\n",
    "* Un notebook de l'analyse exploratoire (non cleané, pour comprendre votre démarche).\n",
    "* Un notebook (ou code commenté au choix) d’essais des différentes approches de modélisation (non cleané, pour comprendre votre démarche).\n",
    "* Un support de présentation pour la soutenance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install geopandas\n",
    "# !pip install plotly\n",
    "# !pip install folium\n",
    "# !pip install ipyleaflet\n",
    "# !pip install bokeh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import et données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:01:09.801718Z",
     "start_time": "2020-01-16T07:01:05.982016Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import duckdb as ddb\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "# Utilisation de Geopandas (pour les données géospatiales) :\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option(\"display.max_rows\", 101)\n",
    "pd.options.display.max_columns = 999\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:01:12.452563Z",
     "start_time": "2020-01-16T07:01:09.803430Z"
    }
   },
   "outputs": [],
   "source": [
    "customers = pd.read_csv('data/olist_customers_dataset.csv')\n",
    "geolocalisation = pd.read_csv('data/olist_geolocation_dataset.csv')\n",
    "order_items = pd.read_csv('data/olist_order_items_dataset.csv')\n",
    "order_payments = pd.read_csv('data/olist_order_payments_dataset.csv')\n",
    "order_reviews = pd.read_csv('data/olist_order_reviews_dataset.csv')\n",
    "orders = pd.read_csv('data/olist_orders_dataset.csv')\n",
    "products = pd.read_csv('data/olist_products_dataset.csv')\n",
    "sellers = pd.read_csv('data/olist_sellers_dataset.csv')\n",
    "translation = pd.read_csv('data/product_category_name_translation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition d'une fonction pour tracer une heatmap\n",
    "# paramètres :\n",
    "# corr : matrice de corrélation\n",
    "# title : titre du graphe\n",
    "# figsize : taille de la figure\n",
    "# vmin : valeur minimale de la colormap\n",
    "# vmax : valeur maximale de la colormap\n",
    "# center : valeur centrale de la colormap\n",
    "# palette : palette de couleurs\n",
    "# shape : forme de la heatmap (rectangle ou triangle)\n",
    "# fmt : format des nombres affichés\n",
    "# robust : booléen pour utiliser une méthode de calcul robuste ou non\n",
    "def plot_heatmap(corr, title, figsize=(8,4), vmin=-1, vmax=1, center=0,\n",
    "                 palette = sns.color_palette(\"coolwarm\", 20), shape='rect',\n",
    "                 fmt='.2f', robust=False):\n",
    "    \n",
    "    # Création d'une figure et d'un axe\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Définition du masque pour la forme de la heatmap\n",
    "    if shape == 'rect':\n",
    "        mask=None\n",
    "    elif shape == 'tri':\n",
    "        mask = np.zeros_like(corr, dtype=bool)\n",
    "        mask[np.triu_indices_from(mask)] = True\n",
    "    else:\n",
    "        print('ERROR : this type of heatmap does not exist')\n",
    "\n",
    "    # Définition de la palette de couleurs\n",
    "    palette = palette\n",
    "    \n",
    "    # Tracé de la heatmap\n",
    "    ax = sns.heatmap(corr, mask=mask, cmap=palette, vmin=vmin, vmax=vmax,\n",
    "                     center=center, annot=True, annot_kws={\"size\": 10},fmt=fmt,\n",
    "                     square=False, linewidths=.5, linecolor = 'white',\n",
    "                     cbar_kws={\"shrink\": .9, 'label': None}, robust = robust,\n",
    "                     xticklabels= corr.columns, yticklabels = corr.index)\n",
    "    \n",
    "    # Configuration des axes et de la légende\n",
    "    ax.tick_params(labelsize=8,top=False, bottom=True,\n",
    "                labeltop=False, labelbottom=True)\n",
    "    ax.collections[0].colorbar.ax.tick_params(labelsize=8)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=25, ha=\"right\",rotation_mode=\"anchor\")\n",
    "    ax.set_title(title, fontweight='bold', fontsize=14)\n",
    "    \n",
    "    # Retourne le masque (inutilisé dans ce code)\n",
    "    # return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def show_correlogram(data):\n",
    "#     numeric_columns = data.select(cs.numeric())\n",
    "#     corr = numeric_columns.to_pandas().corr()\n",
    "#     mask = np.triu(corr)\n",
    "#     sns.set(rc={\"figure.figsize\": (20, 10)})\n",
    "#     sns.heatmap(corr, annot=True, fmt='.2f', \n",
    "#                 # mask=mask,\n",
    "#                 vmin=-1, vmax=1, center=0, cmap='coolwarm')\n",
    "def show_correlogram(data):\n",
    "    numeric_columns = data.select_dtypes(include='number')\n",
    "    corr = numeric_columns.corr()\n",
    "    mask = np.triu(corr)\n",
    "    sns.set(rc={\"figure.figsize\": (20, 10)})\n",
    "#     sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\", mask=mask)\n",
    "    sns.heatmap(corr, annot=True, fmt='.2f', \n",
    "                # mask=mask,\n",
    "                vmin=-1, vmax=1, center=0, cmap='coolwarm')\n",
    "    plt.title(\"Corrélogramme des colonnes numériques\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connaissance du jeu de données et Nettoyage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contenu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:01:12.499362Z",
     "start_time": "2020-01-16T07:01:12.456028Z"
    }
   },
   "outputs": [],
   "source": [
    "customers.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:01:12.531949Z",
     "start_time": "2020-01-16T07:01:12.504623Z"
    }
   },
   "outputs": [],
   "source": [
    "geolocalisation.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:01:12.565703Z",
     "start_time": "2020-01-16T07:01:12.537636Z"
    }
   },
   "outputs": [],
   "source": [
    "order_items.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:01:12.611158Z",
     "start_time": "2020-01-16T07:01:12.570650Z"
    }
   },
   "outputs": [],
   "source": [
    "order_payments.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:01:12.656050Z",
     "start_time": "2020-01-16T07:01:12.613388Z"
    }
   },
   "outputs": [],
   "source": [
    "order_reviews.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:01:12.702317Z",
     "start_time": "2020-01-16T07:01:12.660345Z"
    }
   },
   "outputs": [],
   "source": [
    "orders.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:01:12.749005Z",
     "start_time": "2020-01-16T07:01:12.706824Z"
    }
   },
   "outputs": [],
   "source": [
    "products.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:01:12.784561Z",
     "start_time": "2020-01-16T07:01:12.753335Z"
    }
   },
   "outputs": [],
   "source": [
    "sellers.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:01:12.818024Z",
     "start_time": "2020-01-16T07:01:12.789140Z"
    }
   },
   "outputs": [],
   "source": [
    "translation.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Fichier \"customers.csv\" :\n",
    "   - Résumé : Contient des informations sur les clients qui ont effectué des achats auprès d'Olist.\n",
    "   - Colonnes importantes :\n",
    "     - customer_id : Identifiant unique du client.\n",
    "     - customer_unique_id : Identifiant unique du client, utilisé pour relier les informations du client à ses commandes.\n",
    "     - customer_zip_code_prefix : Code postal du client.\n",
    "     - customer_city : Ville du client.\n",
    "     - customer_state : État du client.\n",
    "\n",
    "2. Fichier \"geolocation.csv\" :\n",
    "   - Résumé : Contient les informations de géolocalisation des différents codes postaux du Brésil.\n",
    "   - Colonnes importantes :\n",
    "     - geolocation_zip_code_prefix : Code postal.\n",
    "     - geolocation_lat : Latitude géographique.\n",
    "     - geolocation_lng : Longitude géographique.\n",
    "     - geolocation_city : Ville correspondante au code postal.\n",
    "     - geolocation_state : État correspondant au code postal.\n",
    "\n",
    "3. Fichier \"order_items.csv\" :\n",
    "   - Résumé : Contient des informations détaillées sur les articles inclus dans chaque commande.\n",
    "   - Colonnes importantes :\n",
    "     - order_id : Identifiant unique de la commande.\n",
    "     - product_id : Identifiant unique du produit.\n",
    "     - seller_id : Identifiant unique du vendeur.\n",
    "     - shipping_limit_date : Date limite d'expédition du produit.\n",
    "\n",
    "4. Fichier \"order_payments.csv\" :\n",
    "   - Résumé : Contient les informations sur les paiements effectués pour chaque commande.\n",
    "   - Colonnes importantes :\n",
    "     - order_id : Identifiant unique de la commande.\n",
    "     - payment_sequential : Numéro de séquence du paiement pour une commande donnée.\n",
    "     - payment_type : Type de paiement utilisé (carte de crédit, virement bancaire, etc.).\n",
    "     - payment_installments : Nombre d'installments (versements) pour le paiement.\n",
    "     - payment_value : Montant total du paiement.\n",
    "\n",
    "5. Fichier \"order_reviews.csv\" :\n",
    "   - Résumé : Contient les avis et évaluations laissés par les clients pour chaque commande.\n",
    "   - Colonnes importantes :\n",
    "     - review_id : Identifiant unique de l'avis.\n",
    "     - order_id : Identifiant unique de la commande associée à l'avis.\n",
    "     - review_score : Note donnée par le client (de 1 à 5).\n",
    "     - review_comment_title : Titre du commentaire laissé par le client.\n",
    "     - review_comment_message : Contenu du commentaire laissé par le client.\n",
    "\n",
    "6. Fichier \"orders.csv\" :\n",
    "   - Résumé : Contient des informations générales sur les commandes passées par les clients.\n",
    "   - Colonnes importantes :\n",
    "     - order_id : Identifiant unique de la commande.\n",
    "     - customer_id : Identifiant unique du client.\n",
    "     - order_status : Statut de la commande (livrée, en cours de traitement, etc.).\n",
    "     - order_purchase_timestamp : Date et heure d'achat de la commande.\n",
    "     - order_approved_at : Date et heure d'approbation de la commande.\n",
    "     - order_delivered_customer_date : Date de livraison estimée au client.\n",
    "     - order_estimated_delivery_date : Date estimée de livraison.\n",
    "\n",
    "7. Fichier \"products.csv\" :\n",
    "   - Résumé : Contient des informations sur les produits vendus par Olist.\n",
    "   - Colonnes importantes :\n",
    "     - product_id : Identifiant unique du produit.\n",
    "     - product_category_name : Nom de la catégorie de produit.\n",
    "     - product_name_length : Longueur du nom du produit.\n",
    "     - product_description_length : Longueur de la description du produit.\n",
    "\n",
    "8. Fichier \"sellers.csv\" :\n",
    "   - Résumé : Contient des informations sur les vendeurs partenaires d'Olist.\n",
    "   - Colonnes importantes :\n",
    "     - seller_id : Identifiant unique du vendeur.\n",
    "     - seller_zip_code_prefix : Code postal du vendeur.\n",
    "     - seller_city : Ville du vendeur.\n",
    "     - seller_state : État du vendeur.\n",
    "\n",
    "9. Fichier \"product_category_name_translation.csv\" :\n",
    "   - Résumé : Contient les traductions des noms de catégories de produits en différentes langues.\n",
    "   - Colonnes importantes :\n",
    "     - product_category_name : Nom de la catégorie de produit en portugais (à traduire en anglais).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:01:12.849193Z",
     "start_time": "2020-01-16T07:01:12.824434Z"
    }
   },
   "outputs": [],
   "source": [
    "liste_df = [customers, \n",
    "            geolocalisation,\n",
    "            order_items,\n",
    "            order_payments,\n",
    "            order_reviews,\n",
    "            orders,products,\n",
    "            sellers,\n",
    "            translation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:01:12.864666Z",
     "start_time": "2020-01-16T07:01:12.857969Z"
    }
   },
   "outputs": [],
   "source": [
    "noms_df = ['customers', \n",
    "           'geolocalisation',\n",
    "           'order_items',\n",
    "           'order_payments',\n",
    "           'order_reviews',\n",
    "           'orders',\n",
    "           'products',\n",
    "           'sellers',\n",
    "           'translation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensions des jeux de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:01:12.896494Z",
     "start_time": "2020-01-16T07:01:12.869362Z"
    }
   },
   "outputs": [],
   "source": [
    "for df in liste_df :\n",
    "    print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for df in liste_df:\n",
    "#     nom_dataframe = noms_df[df\n",
    "    \n",
    "    print(f\"Le dataframe {noms_df[i]} a {df.shape[0]} lignes et {df.shape[1]} colonnes.\")\n",
    "    i = i + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Données manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:01:13.267795Z",
     "start_time": "2020-01-16T07:01:12.901509Z"
    }
   },
   "outputs": [],
   "source": [
    "# for df in liste_df:\n",
    "#     print(df.isna().sum().sum()/df.shape[0]/df.shape[1], 'NaN')\n",
    "# Parcours des DataFrames dans la liste 'liste_df'\n",
    "for df in liste_df:\n",
    "    # Calcule le pourcentage de valeurs NaN dans chaque DataFrame\n",
    "    # isna().sum() compte le nombre de valeurs manquantes dans chaque colonne\n",
    "    # sum() somme le nombre de valeurs manquantes dans toutes les colonnes\n",
    "    # df.shape[0] donne le nombre de lignes dans le DataFrame\n",
    "    # df.shape[1] donne le nombre de colonnes dans le DataFrame\n",
    "    # On divise le nombre total de valeurs manquantes par le nombre total de valeurs dans le DataFrame\n",
    "    # pour obtenir le pourcentage de valeurs NaN dans le DataFrame\n",
    "    pourcentage_nan = df.isna().sum().sum() / (df.shape[0] * df.shape[1])\n",
    "    \n",
    "    # Affiche le pourcentage de valeurs NaN dans le DataFrame\n",
    "    print(pourcentage_nan, 'NaN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code parcourt chaque DataFrame dans la liste liste_df et pour chaque DataFrame, il calcule le pourcentage de valeurs manquantes (NaN) en comptant le nombre total de valeurs manquantes dans toutes les colonnes et en le divisant par le nombre total de valeurs dans le DataFrame (nombre de lignes multiplié par le nombre de colonnes). Ensuite, il affiche le pourcentage de valeurs NaN pour chaque DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parcours des DataFrames dans la liste 'liste_df'\n",
    "# for df in liste_df:\n",
    "#     # Calcule le pourcentage de valeurs NaN dans chaque DataFrame\n",
    "#     # isna().sum() compte le nombre de valeurs manquantes dans chaque colonne\n",
    "#     # sum() somme le nombre de valeurs manquantes dans toutes les colonnes\n",
    "#     # df.shape[0] donne le nombre de lignes dans le DataFrame\n",
    "#     # df.shape[1] donne le nombre de colonnes dans le DataFrame\n",
    "#     # On divise le nombre total de valeurs manquantes par le nombre total de valeurs dans le DataFrame\n",
    "#     # pour obtenir le pourcentage de valeurs NaN dans le DataFrame\n",
    "#     pourcentage_nan = df.isna().sum().sum() / (df.shape[0] * df.shape[1])\n",
    "    \n",
    "#     # Affiche le pourcentage de valeurs NaN dans le DataFrame avec le libellé du DataFrame\n",
    "#     print(f\"Pourcentage de valeurs NaN dans le DataFrame '{df}' : {pourcentage_nan:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a des NaN uniquement des order_reviews, orders et products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:01:13.342274Z",
     "start_time": "2020-01-16T07:01:13.272849Z"
    }
   },
   "outputs": [],
   "source": [
    "# order_reviews.isna().sum(axis=0)\n",
    "# Calculer le nombre de valeurs manquantes dans chaque colonne du DataFrame 'order_reviews'\n",
    "# La méthode isna() retourne un DataFrame avec des valeurs booléennes (True pour les valeurs manquantes, False sinon)\n",
    "# La méthode sum() retourne la somme des valeurs booléennes pour chaque colonne (True est équivalent à 1, False est équivalent à 0)\n",
    "# L'argument axis=0 spécifie que la somme doit être effectuée par colonne\n",
    "# Ainsi, le résultat sera le nombre de valeurs manquantes dans chaque colonne du DataFrame 'order_reviews'\n",
    "order_reviews_missing_values = order_reviews.isna().sum(axis=0)\n",
    "\n",
    "# Afficher le nombre de valeurs manquantes dans chaque colonne du DataFrame 'order_reviews'\n",
    "print(order_reviews_missing_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code calcule le nombre de valeurs manquantes dans chaque colonne du DataFrame order_reviews en utilisant la méthode isna() pour identifier les valeurs manquantes et la méthode sum() pour compter le nombre de valeurs manquantes dans chaque colonne. Ensuite, il affiche le nombre de valeurs manquantes pour chaque colonne du DataFrame order_reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pour order_reviews, les NaN correspondent à  des commentaires vides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:01:13.423882Z",
     "start_time": "2020-01-16T07:01:13.347981Z"
    }
   },
   "outputs": [],
   "source": [
    "# orders.isna().sum(axis=0)\n",
    "# Calculer le nombre de valeurs manquantes dans chaque colonne du DataFrame 'orders'\n",
    "# La méthode isna() retourne un DataFrame avec des valeurs booléennes (True pour les valeurs manquantes, False sinon)\n",
    "# La méthode sum() retourne la somme des valeurs booléennes pour chaque colonne (True est équivalent à 1, False est équivalent à 0)\n",
    "# L'argument axis=0 spécifie que la somme doit être effectuée par colonne\n",
    "# Ainsi, le résultat sera le nombre de valeurs manquantes dans chaque colonne du DataFrame 'orders'\n",
    "orders_missing_values = orders.isna().sum(axis=0)\n",
    "\n",
    "# Afficher le nombre de valeurs manquantes dans chaque colonne du DataFrame 'orders'\n",
    "print(orders_missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code calcule le nombre de valeurs manquantes dans chaque colonne du DataFrame orders en utilisant la méthode isna() pour identifier les valeurs manquantes et la méthode sum() pour compter le nombre de valeurs manquantes dans chaque colonne. Ensuite, il affiche le nombre de valeurs manquantes pour chaque colonne du DataFrame orders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour les commandes, les NaN sont croissants dans l'ordre d'un processus de commande et correspondent aux commandes qui posent problème"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:01:13.820428Z",
     "start_time": "2020-01-16T07:01:13.426950Z"
    }
   },
   "outputs": [],
   "source": [
    "# orders.max(\n",
    "# Trouver la valeur maximale dans chaque colonne du DataFrame 'orders'\n",
    "# La méthode max() retourne la valeur maximale dans chaque colonne du DataFrame\n",
    "# Si les colonnes contiennent des valeurs numériques, la méthode max() renverra la valeur numérique maximale\n",
    "# Si les colonnes contiennent des valeurs non numériques, la méthode max() renverra la valeur maximale dans l'ordre lexicographique\n",
    "orders_max_values = orders.max()\n",
    "\n",
    "# Afficher la valeur maximale dans chaque colonne du DataFrame 'orders'\n",
    "print(orders_max_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code trouve la valeur maximale dans chaque colonne du DataFrame orders en utilisant la méthode max(). Si les colonnes contiennent des valeurs numériques, la méthode max() renverra la valeur numérique maximale dans chaque colonne. Si les colonnes contiennent des valeurs non numériques, la méthode max() renverra la valeur maximale dans l'ordre lexicographique. Ensuite, il affiche la valeur maximale dans chaque colonne du DataFrame orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:01:13.847398Z",
     "start_time": "2020-01-16T07:01:13.822982Z"
    }
   },
   "outputs": [],
   "source": [
    "orders.order_approved_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:01:13.874095Z",
     "start_time": "2020-01-16T07:01:13.850563Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# products.isna().sum(axis=0)\n",
    "# Calculer le nombre de valeurs manquantes dans chaque colonne du DataFrame 'products'\n",
    "# La méthode isna() retourne un DataFrame avec des valeurs booléennes (True pour les valeurs manquantes, False sinon)\n",
    "# La méthode sum() retourne la somme des valeurs booléennes pour chaque colonne (True est équivalent à 1, False est équivalent à 0)\n",
    "# L'argument axis=0 spécifie que la somme doit être effectuée par colonne\n",
    "# Ainsi, le résultat sera le nombre de valeurs manquantes dans chaque colonne du DataFrame 'products'\n",
    "products_missing_values = products.isna().sum(axis=0)\n",
    "\n",
    "# Afficher le nombre de valeurs manquantes dans chaque colonne du DataFrame 'products'\n",
    "print(products_missing_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:01:13.899845Z",
     "start_time": "2020-01-16T07:01:13.877002Z"
    }
   },
   "outputs": [],
   "source": [
    "products.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:01:13.948261Z",
     "start_time": "2020-01-16T07:01:13.902253Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "products[products['product_category_name'].isna()].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a des produits pour lesquels il manque des informations importantes (en particulier la catégorie de produit).\n",
    "\n",
    "On pourra éventuellement leur donner des attributs correspondant à leur caractère inconnu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:01:13.991380Z",
     "start_time": "2020-01-16T07:01:13.953626Z"
    }
   },
   "outputs": [],
   "source": [
    "# products[products['product_weight_g'].isna()]\n",
    "# Sélectionner les lignes du DataFrame 'products' où la colonne 'product_weight_g' contient des valeurs manquantes (NaN)\n",
    "# La condition products['product_weight_g'].isna() renvoie une série booléenne indiquant True pour les valeurs manquantes et False sinon\n",
    "# En utilisant cette série booléenne comme masque, nous sélectionnons les lignes où la colonne 'product_weight_g' est manquante\n",
    "products_missing_weight = products[products['product_weight_g'].isna()]\n",
    "\n",
    "# Afficher les lignes du DataFrame 'products' où la colonne 'product_weight_g' est manquante\n",
    "products_missing_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### doublons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:01:15.467899Z",
     "start_time": "2020-01-16T07:01:13.994359Z"
    }
   },
   "outputs": [],
   "source": [
    "# for df in liste_df:\n",
    "#     print(df.duplicated().sum())\n",
    "# Pour chaque DataFrame 'df' dans la liste 'liste_df'\n",
    "for df in liste_df:\n",
    "    # Calculer le nombre de lignes dupliquées dans le DataFrame 'df'\n",
    "    # La méthode 'duplicated()' renvoie une série booléenne indiquant True pour les lignes dupliquées et False sinon\n",
    "    # En utilisant la méthode 'sum()', nous comptons le nombre total de lignes dupliquées en sommant les valeurs True (1) dans la série\n",
    "    nb_duplicates = df.duplicated().sum()\n",
    "\n",
    "    # Afficher le nombre de lignes dupliquées dans le DataFrame 'df'\n",
    "    print(nb_duplicates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Types des données et mode des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i=0\n",
    "# liste_indices = []\n",
    "# liste_colonnes = []\n",
    "# liste_types = []\n",
    "# liste_uniques = []\n",
    "\n",
    "# for df in liste_df:\n",
    "\n",
    "#     for column in df.columns:\n",
    "#         liste_indices.append(noms_df[i])\n",
    "#         liste_colonnes.append(column)\n",
    "#         liste_types.append(df[column].dtype)\n",
    "#         liste_uniques.append(df[column].nunique())\n",
    "#     i+=1\n",
    "# resume_data = pd.DataFrame([liste_indices, \n",
    "#                             liste_colonnes, \n",
    "#                             liste_types, \n",
    "#                             liste_uniques]\n",
    "#                           ).T\n",
    "# resume_data.columns=['Donnees', 'Nom colonne', 'Type', 'valeurs uniques']\n",
    "# resume_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:01:16.368957Z",
     "start_time": "2020-01-16T07:01:15.471813Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialisation des listes qui contiendront les informations résumées\n",
    "i = 0\n",
    "liste_indices = []\n",
    "liste_colonnes = []\n",
    "liste_types = []\n",
    "liste_uniques = []\n",
    "\n",
    "# Parcours de chaque DataFrame 'df' dans la liste 'liste_df'\n",
    "for df in liste_df:\n",
    "    # Parcours de chaque colonne 'column' dans le DataFrame 'df'\n",
    "    for column in df.columns:\n",
    "        # Ajout des informations de chaque colonne dans les listes correspondantes\n",
    "        liste_indices.append(noms_df[i])   # Nom du DataFrame 'df' correspondant (utilisation de noms_df[i])\n",
    "        liste_colonnes.append(column)      # Nom de la colonne\n",
    "        liste_types.append(df[column].dtype)  # Type de données de la colonne\n",
    "        liste_uniques.append(df[column].nunique())  # Nombre de valeurs uniques dans la colonne\n",
    "\n",
    "    i += 1  # Passage au DataFrame suivant\n",
    "\n",
    "# Création d'un DataFrame résumant les informations collectées\n",
    "resume_data = pd.DataFrame([liste_indices, liste_colonnes, liste_types, liste_uniques]).T\n",
    "resume_data.columns = ['Donnees', 'Nom colonne', 'Type', 'Valeurs uniques']\n",
    "\n",
    "# Affichage du DataFrame résumé\n",
    "resume_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code parcourt chaque DataFrame de la liste liste_df, puis chaque colonne de chaque DataFrame, et collecte les informations telles que le nom du DataFrame correspondant, le nom de la colonne, le type de données de la colonne et le nombre de valeurs uniques dans la colonne. Ces informations sont ensuite rassemblées dans un DataFrame résumé appelé resume_data, qui est ensuite affiché."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* customers\n",
    "     * davantage de valeurs différentes pour customer_id que customer_unique_id. \n",
    "     * customer_state pourrait être catéogrisé\n",
    "* geolocalisation\n",
    "    * plus de villes que dans la base de données customers :\n",
    "    * geolocation_state pourrait être catégorisé\n",
    "* order_items\n",
    "    * order_item_id contient 21 types différents. \n",
    "    * autant de seller_id que dans la base ed données sellers\n",
    "    * shipping_limit_date : à mettre en format date\n",
    "* order_payments :\n",
    "    * order_id contient autant de valeurs que dans order\n",
    "    * payment_sequential et payment_type pourraient être catégories\n",
    "* orders \n",
    "    * order_purchase_timestamp : à mettre en format date\n",
    "    * order_approved_at :  à mettre en format date\n",
    "    * order_delivered_carrier_date :  à mettre en format date\n",
    "    * order_delivered_customer_date :  à mettre en format date\n",
    "    * order_estimated_delivery_date :  à mettre en format date\n",
    "* products:\n",
    "    * 73 catégories différentes\n",
    "* sellers:\n",
    "* translation : 3 catégories non traduites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonction de nettoyage "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### contrôle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:01:16.400338Z",
     "start_time": "2020-01-16T07:01:16.371767Z"
    }
   },
   "outputs": [],
   "source": [
    "# def controle_df(liste_donnees):\n",
    "#     '''Vérifications de type et de la taille du jeu de données pour correspondre \n",
    "#     au jeu initial\n",
    "    \n",
    "#     Prend en entrée une liste de dataframes\n",
    "#     Retourne un Booléen\n",
    "#     True dans le cas d'une liste de 9 objects de type dataframe\n",
    "#     False dans le cas contraire\n",
    "#     '''\n",
    "#     if type(liste_donnees) is list:\n",
    "#         if len(liste_donnees) == 9:\n",
    "#             for df in liste_donnees:\n",
    "#                 if type(df) is not type(pd.DataFrame()):\n",
    "#                     return False\n",
    "#         else:\n",
    "#             return False\n",
    "#     else:\n",
    "#         return False\n",
    "#     return True\n",
    "def controle_df(liste_donnees):\n",
    "    '''\n",
    "    Vérifications de type et de la taille du jeu de données pour correspondre \n",
    "    au jeu initial\n",
    "    \n",
    "    Prend en entrée une liste de dataframes\n",
    "    Retourne un Booléen\n",
    "    True dans le cas d'une liste de 9 objets de type dataframe\n",
    "    False dans le cas contraire\n",
    "    '''\n",
    "    # Vérifie si la variable 'liste_donnees' est de type liste\n",
    "    if type(liste_donnees) is list:\n",
    "        # Vérifie si la liste contient 9 éléments (dataframes)\n",
    "        if len(liste_donnees) == 9:\n",
    "            # Parcours chaque dataframe 'df' dans la liste 'liste_donnees'\n",
    "            for df in liste_donnees:\n",
    "                # Vérifie si chaque élément de la liste est bien un objet de type dataframe\n",
    "                if type(df) is not type(pd.DataFrame()):\n",
    "                    # Si ce n'est pas le cas, renvoie False (la liste ne contient pas 9 dataframes)\n",
    "                    return False\n",
    "        else:\n",
    "            # Si la liste ne contient pas 9 éléments, renvoie False\n",
    "            return False\n",
    "    else:\n",
    "        # Si 'liste_donnees' n'est pas de type liste, renvoie False\n",
    "        return False\n",
    "    \n",
    "    # Si toutes les vérifications sont passées, renvoie True (la liste contient bien 9 dataframes)\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fonction controle_df prend en entrée une liste de dataframes, et elle vérifie si la liste contient bien 9 objets de type dataframe. Si c'est le cas, elle renvoie True, sinon elle renvoie False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nettoyage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:01:16.452259Z",
     "start_time": "2020-01-16T07:01:16.403990Z"
    }
   },
   "outputs": [],
   "source": [
    "# def nettoyage(liste_donnees):\n",
    "#     '''Nettoyage des différents dataframe\n",
    "#     Entrée : liste de 9 dataframes\n",
    "    \n",
    "#     Traitement appliqué : \n",
    "#     * complétion des NaN, \n",
    "#     * nettoyage type, \n",
    "#     * merge catégories de produits\n",
    "    \n",
    "#     Return : liste des dataframe nettoyée\n",
    "#     '''\n",
    "#     if not controle_df(liste_donnees):\n",
    "#         return False\n",
    "\n",
    "#     customers_local = liste_donnees[0].copy()\n",
    "#     geolocalisation_local  = liste_donnees[1].copy()\n",
    "#     order_items_local  = liste_donnees[2].copy()\n",
    "#     order_payments_local  = liste_donnees[3].copy()\n",
    "#     order_reviews_local  = liste_donnees[4].copy()\n",
    "#     orders_local  = liste_donnees[5].copy()\n",
    "#     products_local = liste_donnees[6].copy()\n",
    "#     sellers_local = liste_donnees[7].copy()\n",
    "#     translation_local = liste_donnees[8].copy()\n",
    "\n",
    "    \n",
    "#     #Traitement des NaN\n",
    "#     geolocalisation_local.drop_duplicates(inplace=True)\n",
    "    \n",
    "#     order_reviews_local['review_comment_title'].fillna(' ',\n",
    "#                                                        inplace=True)\n",
    "#     order_reviews_local['review_comment_message'].fillna(' ',\n",
    "#                                                          inplace=True)\n",
    "    \n",
    "#     orders['order_approved_at'].fillna(-1, inplace=True)\n",
    "#     orders['order_delivered_carrier_date'].fillna(0, inplace=True)\n",
    "#     orders['order_delivered_customer_date'].fillna(0, inplace=True)\n",
    "    \n",
    "#     products_local['product_category_name'].fillna('Unkwown', \n",
    "#                                                    inplace=True)\n",
    "#     products_local['product_name_lenght'].fillna(0, inplace=True)\n",
    "#     products_local['product_description_lenght'].fillna(0, \n",
    "#                                                         inplace=True)\n",
    "#     products_local['product_photos_qty'].fillna(0, inplace=True)\n",
    "#     products_local['product_weight_g'].fillna(0, inplace=True)\n",
    "#     products_local['product_length_cm'].fillna(0, inplace=True)\n",
    "#     products_local['product_height_cm'].fillna(0, inplace=True)\n",
    "#     products_local['product_width_cm'].fillna(0, inplace=True)\n",
    "    \n",
    "#     #nettoyage types:\n",
    "#     order_items_local['shipping_limit_date'] = order_items_local[\n",
    "#         'shipping_limit_date'].astype('datetime64')\n",
    "#     orders_local['order_purchase_timestamp'] = orders_local[\n",
    "#         'order_purchase_timestamp'].astype('datetime64')\n",
    "#     orders_local['order_approved_at'] = pd.to_datetime(orders_local[\n",
    "#         'order_approved_at'], errors='coerce')\n",
    "#     orders_local['order_delivered_carrier_date'] = pd.to_datetime(\n",
    "#         orders_local['order_delivered_carrier_date'], errors='coerce')\n",
    "#     orders_local['order_delivered_customer_date'] = pd.to_datetime(\n",
    "#         orders_local['order_delivered_customer_date'], errors='coerce')\n",
    "#     orders_local['order_estimated_delivery_date'] = orders_local[\n",
    "#         'order_estimated_delivery_date'].astype('datetime64')\n",
    "    \n",
    "#     #merge des catégories de produits\n",
    "#     products_local = pd.merge(products_local, translation_local).drop(\n",
    "#         ['product_category_name'], axis=1)\n",
    "    \n",
    "#     return [\n",
    "#         customers_local, \n",
    "#         geolocalisation_local, \n",
    "#         order_items_local, \n",
    "#         order_payments_local, \n",
    "#         order_reviews_local, \n",
    "#         orders_local, \n",
    "#         products_local, \n",
    "#         sellers_local, \n",
    "#         translation_local]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nettoyage(liste_donnees):\n",
    "    '''\n",
    "    Nettoyage des différents dataframes\n",
    "    Entrée : liste de 9 dataframes\n",
    "    \n",
    "    Traitement appliqué : \n",
    "    * complétion des NaN, \n",
    "    * nettoyage des types de données, \n",
    "    * fusion des catégories de produits\n",
    "    \n",
    "    Retourne : liste des dataframes nettoyés\n",
    "    '''\n",
    "    # Vérifie si la liste de données est valide en appelant la fonction controle_df\n",
    "    if not controle_df(liste_donnees):\n",
    "        return False\n",
    "\n",
    "    # Effectue une copie des dataframes pour éviter de modifier les dataframes d'origine\n",
    "    customers_local = liste_donnees[0].copy()\n",
    "    geolocalisation_local  = liste_donnees[1].copy()\n",
    "    order_items_local  = liste_donnees[2].copy()\n",
    "    order_payments_local  = liste_donnees[3].copy()\n",
    "    order_reviews_local  = liste_donnees[4].copy()\n",
    "    orders_local  = liste_donnees[5].copy()\n",
    "    products_local = liste_donnees[6].copy()\n",
    "    sellers_local = liste_donnees[7].copy()\n",
    "    translation_local = liste_donnees[8].copy()\n",
    "\n",
    "    # Traitement des NaN\n",
    "\n",
    "    # Supprime les lignes dupliquées dans le dataframe geolocalisation_local\n",
    "    geolocalisation_local.drop_duplicates(inplace=True)\n",
    "\n",
    "    # Remplace les valeurs manquantes dans les colonnes 'review_comment_title' et 'review_comment_message' par des espaces vides\n",
    "    order_reviews_local['review_comment_title'].fillna(' ', inplace=True)\n",
    "    order_reviews_local['review_comment_message'].fillna(' ', inplace=True)\n",
    "\n",
    "    # Remplace les valeurs manquantes dans certaines colonnes du dataframe orders_local\n",
    "    orders['order_approved_at'].fillna(-1, inplace=True)\n",
    "    orders['order_delivered_carrier_date'].fillna(0, inplace=True)\n",
    "    orders['order_delivered_customer_date'].fillna(0, inplace=True)\n",
    "\n",
    "    # Remplace les valeurs manquantes dans certaines colonnes du dataframe products_local par des valeurs par défaut\n",
    "    products_local['product_category_name'].fillna('Unkwown', inplace=True)\n",
    "    products_local['product_name_lenght'].fillna(0, inplace=True)\n",
    "    products_local['product_description_lenght'].fillna(0, inplace=True)\n",
    "    products_local['product_photos_qty'].fillna(0, inplace=True)\n",
    "    products_local['product_weight_g'].fillna(0, inplace=True)\n",
    "    products_local['product_length_cm'].fillna(0, inplace=True)\n",
    "    products_local['product_height_cm'].fillna(0, inplace=True)\n",
    "    products_local['product_width_cm'].fillna(0, inplace=True)\n",
    "\n",
    "    # Nettoyage des types de données:\n",
    "\n",
    "    # Convertit la colonne 'shipping_limit_date' du dataframe order_items_local en type datetime64\n",
    "    order_items_local['shipping_limit_date'] = order_items_local['shipping_limit_date'].astype('datetime64')\n",
    "\n",
    "    # Convertit certaines colonnes du dataframe orders_local en type datetime64\n",
    "    orders_local['order_purchase_timestamp'] = orders_local['order_purchase_timestamp'].astype('datetime64')\n",
    "    orders_local['order_approved_at'] = pd.to_datetime(orders_local['order_approved_at'], errors='coerce')\n",
    "    orders_local['order_delivered_carrier_date'] = pd.to_datetime(orders_local['order_delivered_carrier_date'], errors='coerce')\n",
    "    orders_local['order_delivered_customer_date'] = pd.to_datetime(orders_local['order_delivered_customer_date'], errors='coerce')\n",
    "    orders_local['order_estimated_delivery_date'] = orders_local['order_estimated_delivery_date'].astype('datetime64')\n",
    "\n",
    "    # Fusion des catégories de produits\n",
    "\n",
    "    # Fusionne les dataframes products_local et translation_local en utilisant la colonne 'product_category_name' comme clé de fusion\n",
    "    # Supprime ensuite la colonne 'product_category_name' du dataframe products_local\n",
    "    products_local = pd.merge(products_local, translation_local).drop(['product_category_name'], axis=1)\n",
    "\n",
    "    # Retourne la liste des dataframes nettoyés\n",
    "    return [customers_local, \n",
    "            geolocalisation_local, \n",
    "            order_items_local, \n",
    "            order_payments_local, \n",
    "            order_reviews_local, \n",
    "            orders_local, \n",
    "            products_local, \n",
    "            sellers_local, \n",
    "            translation_local]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fonction `nettoyage` prend en entrée une liste de 9 dataframes et effectue les opérations de nettoyage décrites dans les commentaires. Elle renvoie ensuite une liste contenant les dataframes nettoyés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:01:17.868361Z",
     "start_time": "2020-01-16T07:01:16.459144Z"
    }
   },
   "outputs": [],
   "source": [
    "liste_df = nettoyage(liste_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assemblage, traitement outliers et feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mapping categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:01:17.904085Z",
     "start_time": "2020-01-16T07:01:17.871618Z"
    }
   },
   "outputs": [],
   "source": [
    "# Définition d'un dictionnaire pour regrouper les catégories de produits en catégories générales\n",
    "\n",
    "dict_categories = {\n",
    "    # Catégorie : 'home'\n",
    "    'furniture_living_room': 'home',\n",
    "    'furniture_mattress_and_upholstery': 'home',\n",
    "    'furniture_bedroom': 'home',\n",
    "    'furniture_decor': 'home',\n",
    "    'bed_bath_table': 'home',\n",
    "    'kitchen_dining_laundry_garden_furniture': 'home',\n",
    "    'la_cuisine': 'home',\n",
    "    'home_confort': 'home',\n",
    "    'home_comfort_2': 'home',\n",
    "    'christmas_supplies': 'home',\n",
    "\n",
    "    # Catégorie : 'appliances' = appareils électroménagers\n",
    "    'small_appliances': 'appliances',\n",
    "    'small_appliances_home_oven_and_coffee': 'appliances',\n",
    "    'home_appliances_2': 'appliances',\n",
    "    'home_appliances': 'appliances',\n",
    "    'housewares': 'appliances',\n",
    "\n",
    "    # Catégorie : 'construction'\n",
    "    'construction_tools_construction': 'construction',\n",
    "    'costruction_tools_garden': 'construction',\n",
    "    'costruction_tools_tools': 'construction',\n",
    "    'construction_tools_safety': 'construction',\n",
    "    'construction_tools_lights': 'construction',\n",
    "    'home_construction': 'construction',\n",
    "    'air_conditioning': 'construction',\n",
    "\n",
    "    # Catégorie : 'office'\n",
    "    'office_furniture': 'office',\n",
    "    'industry_commerce_and_business': 'office',\n",
    "    'stationery': 'office',\n",
    "    'agro_industry_and_commerce': 'office',\n",
    "    'signaling_and_security': 'office',\n",
    "    'furnitures': 'office',\n",
    "    'security_and_services': 'office',\n",
    "\n",
    "    # Catégorie : 'electronics'\n",
    "    'telephony': 'electronics',\n",
    "    'electronics': 'electronics',\n",
    "    'computers_accessories': 'electronics',\n",
    "    'consoles_games': 'electronics',\n",
    "    'fixed_telephony': 'electronics',\n",
    "    'audio': 'electronics',\n",
    "    'computers': 'electronics',\n",
    "    'tablets_printing_image': 'electronics',\n",
    "\n",
    "    # Catégorie : 'sports_leisure'\n",
    "    'sports_leisure': 'sports_leisure',\n",
    "    'musical_instruments': 'sports_leisure',\n",
    "    'party_supplies': 'sports_leisure',\n",
    "    'luggage_accessories': 'sports_leisure',\n",
    "\n",
    "    # Catégorie : 'arts'\n",
    "    'books': 'arts',\n",
    "    'books_imported': 'arts',\n",
    "    'books_general_interest': 'arts',\n",
    "    'books_technical': 'arts',\n",
    "    'art': 'arts',\n",
    "    'toys': 'arts',\n",
    "    'cine_photo': 'arts',\n",
    "    'cds_dvds_musicals': 'arts',\n",
    "    'music': 'arts',\n",
    "    'dvds_blu_ray': 'arts',\n",
    "    'arts_and_craftmanship': 'arts',\n",
    "\n",
    "    # Catégorie : 'fashion'\n",
    "    'watches_gifts': 'fashion',\n",
    "    'fashion_bags_accessories': 'fashion',\n",
    "    'fashion_underwear_beach': 'fashion',\n",
    "    'fashion_shoes': 'fashion',\n",
    "    'fashion_male_clothing': 'fashion',\n",
    "    'fashio_female_clothing': 'fashion',\n",
    "    'fashion_sport': 'fashion',\n",
    "    'fashion_childrens_clothes': 'fashion',\n",
    "\n",
    "    # Catégorie : 'health_beauty'\n",
    "    'health_beauty': 'health_beauty',\n",
    "    'baby': 'health_beauty',\n",
    "    'diapers_and_hygiene': 'health_beauty',\n",
    "    'perfumery': 'health_beauty',\n",
    "\n",
    "    # Catégorie : 'garden_pets'\n",
    "    'flowers': 'garden_pets',\n",
    "    'pet_shop': 'garden_pets',\n",
    "    'garden_tools': 'garden_pets',\n",
    "\n",
    "    # Catégorie : 'auto'\n",
    "    'auto': 'auto',\n",
    "\n",
    "    # Catégorie : 'food_drinks'\n",
    "    'food_drink': 'food_drinks',\n",
    "    'food': 'food_drinks',\n",
    "    'drinks': 'food_drinks',\n",
    "\n",
    "    # Catégorie : 'other'\n",
    "    'Unknown': 'other',\n",
    "    'market_place': 'other',\n",
    "    'cool_stuff': 'other'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce dictionnaire dict_categories regroupe les catégories de produits en catégories plus générales. Chaque clé du dictionnaire représente une catégorie de produits spécifique, tandis que les valeurs associées représentent les catégories générales dans lesquelles ces produits sont regroupés. Par exemple, tous les produits liés à la maison sont regroupés sous la catégorie 'home', et tous les produits liés aux appareils sont regroupés sous la catégorie 'appliances', et ainsi de suite. Cela permet de simplifier et de mieux organiser les données relatives aux produits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:01:18.263996Z",
     "start_time": "2020-01-16T07:01:17.906083Z"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "# def delete_univariate_outliers(dataframe):\n",
    "#     '''Suppression des valeurs extrêmes du dataset - on exclut le centile le plus extreme\n",
    "#     Entree: objet dataframe\n",
    "#     Traitement : Supression Nan univariés\n",
    "#     Sortie : objet dataframe\n",
    "#     '''\n",
    "#     #valeurs extremes\n",
    "#     index_nan = []\n",
    "#     index_nan_flat = []\n",
    "#     for column in dataframe.select_dtypes(include = ['int32','float64']).columns.tolist() :\n",
    "\n",
    "        \n",
    "#         index_nan.append(dataframe.loc[dataframe[column] > dataframe[\n",
    "#             column].quantile(0.99)].index.tolist())\n",
    "#         index_nan.append(dataframe.loc[dataframe[column] < dataframe[\n",
    "#             column].quantile(0.01)].index.tolist())\n",
    "\n",
    "#     for sublist in index_nan:\n",
    "#         for item in sublist:\n",
    "#             index_nan_flat.append(item)\n",
    "                \n",
    "#     #suppression des doublons\n",
    "#     index_nan_flat = list(dict.fromkeys(index_nan_flat))\n",
    "#     dataframe[column].loc[index_nan_flat] = np.nan\n",
    "\n",
    "#     return dataframe.dropna(axis=0)\n",
    "\n",
    "# def delete_multivariate_outliers(dataframe):\n",
    "#     '''Suppression des outliers multivariés \n",
    "#     (1% le plus éloigné par le calcul de la distance aux 5 plus proches voisins)\n",
    "#     Entree : objet dataframe\n",
    "#     Sortie : objet dataframe\n",
    "#     '''\n",
    "    \n",
    "#     lof = LocalOutlierFactor(n_neighbors = 5, n_jobs=-1)\n",
    "#     lof.fit_predict(dataframe.select_dtypes(['float64','int32']).dropna())\n",
    "#     indices = dataframe.select_dtypes(['float64','int32']).dropna().index\n",
    "#     df_lof = pd.DataFrame(index = indices,\n",
    "#                            data = lof.negative_outlier_factor_, columns=['lof'])\n",
    "#     index_to_drop = df_lof[df_lof['lof']< np.quantile(\n",
    "#         lof.negative_outlier_factor_, 0.01)].index\n",
    "#     return dataframe.drop(index_to_drop, axis=0)\n",
    "\n",
    "# def clean_outliers(dataframe):\n",
    "#     dataframe = delete_univariate_outliers(dataframe)\n",
    "\n",
    "#     dataframe = delete_multivariate_outliers(dataframe)\n",
    "#     return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation de la fonction LocalOutlierFactor du module sklearn.neighbors\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "# Définition d'une fonction pour supprimer les valeurs extrêmes univariées d'un dataframe\n",
    "def delete_univariate_outliers(dataframe):\n",
    "    '''Suppression des valeurs extrêmes du dataset - on exclut le centile le plus extreme\n",
    "    Entree: objet dataframe\n",
    "    Traitement : Supression Nan univariés\n",
    "    Sortie : objet dataframe\n",
    "    '''\n",
    "    # Initialisation d'une liste pour stocker les index des valeurs extrêmes\n",
    "    index_nan = []\n",
    "    index_nan_flat = []\n",
    "    \n",
    "    # Parcours des colonnes numériques du dataframe\n",
    "    for column in dataframe.select_dtypes(include=['int32', 'float64']).columns.tolist():\n",
    "\n",
    "        # Recherche des index des valeurs supérieures au centile 99 et inférieures au centile 1\n",
    "        index_nan.append(dataframe.loc[dataframe[column] > dataframe[column].quantile(0.99)].index.tolist())\n",
    "        index_nan.append(dataframe.loc[dataframe[column] < dataframe[column].quantile(0.01)].index.tolist())\n",
    "\n",
    "    # Aplatir la liste des index des valeurs extrêmes\n",
    "    for sublist in index_nan:\n",
    "        for item in sublist:\n",
    "            index_nan_flat.append(item)\n",
    "                \n",
    "    # Suppression des doublons dans la liste des index\n",
    "    index_nan_flat = list(dict.fromkeys(index_nan_flat))\n",
    "    \n",
    "    # Remplacement des valeurs extrêmes par NaN dans le dataframe\n",
    "    for column in dataframe.select_dtypes(include=['int32', 'float64']).columns.tolist():\n",
    "        dataframe[column].loc[index_nan_flat] = np.nan\n",
    "\n",
    "    # Suppression des lignes contenant des valeurs NaN\n",
    "    return dataframe.dropna(axis=0)\n",
    "\n",
    "# Définition d'une fonction pour supprimer les outliers multivariés d'un dataframe\n",
    "def delete_multivariate_outliers(dataframe):\n",
    "    '''Suppression des outliers multivariés \n",
    "    (1% le plus éloigné par le calcul de la distance aux 5 plus proches voisins)\n",
    "    Entree : objet dataframe\n",
    "    Sortie : objet dataframe\n",
    "    '''\n",
    "    # Création d'un modèle Local Outlier Factor (LOF) avec 5 voisins\n",
    "    lof = LocalOutlierFactor(n_neighbors=5, n_jobs=-1)\n",
    "    \n",
    "    # Entraînement du modèle et prédiction des outliers\n",
    "    lof.fit_predict(dataframe.select_dtypes(['float64', 'int32']).dropna())\n",
    "    \n",
    "    # Récupération des indices des outliers\n",
    "    indices = dataframe.select_dtypes(['float64', 'int32']).dropna().index\n",
    "    df_lof = pd.DataFrame(index=indices, data=lof.negative_outlier_factor_, columns=['lof'])\n",
    "    \n",
    "    # Suppression des outliers dont le score LOF est inférieur au 1er centile\n",
    "    index_to_drop = df_lof[df_lof['lof'] < np.quantile(lof.negative_outlier_factor_, 0.01)].index\n",
    "    return dataframe.drop(index_to_drop, axis=0)\n",
    "\n",
    "# Définition d'une fonction pour nettoyer les outliers d'un dataframe\n",
    "def clean_outliers(dataframe):\n",
    "    # Suppression des outliers univariés\n",
    "    dataframe = delete_univariate_outliers(dataframe)\n",
    "\n",
    "    # Suppression des outliers multivariés\n",
    "    dataframe = delete_multivariate_outliers(dataframe)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Ces fonctions permettent de nettoyer les valeurs aberrantes dans un dataframe en supprimant les valeurs extrêmes univariées et les outliers multivariés. Les valeurs extrêmes univariées sont supprimées en remplaçant les valeurs qui dépassent les centiles 1 et 99 par des NaN. Les outliers multivariés sont identifiés en utilisant le modèle LOF (Local Outlier Factor) et sont supprimés s'ils ont un score LOF inférieur au 1er centile. Le dataframe nettoyé est ensuite renvoyé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fonction de nettoyage globale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:01:18.321089Z",
     "start_time": "2020-01-16T07:01:18.264795Z"
    }
   },
   "outputs": [],
   "source": [
    "# Définition d'une fonction pour appliquer le traitement complet et créer de nouvelles features à partir d'une liste de données\n",
    "def apply_features(liste_donnees):\n",
    "    '''Application traitement complet et création de nouvelles features\n",
    "    \n",
    "    Traitement:\n",
    "    * Création de nouvelles features\n",
    "    * Merge de l'ensemble des dataframe dans un seul dataframe sur la clé client unique\n",
    "    * Nettoyage des outliers\n",
    "    \n",
    "    Entree : liste d'objets \n",
    "    Sortie : dataframe cleané\n",
    "    '''\n",
    "    print('### Création features ###')\n",
    "    # Récupération des dataframes à partir de la liste de données\n",
    "    customers_local = liste_donnees[0].copy()\n",
    "    geolocalisation_local = liste_donnees[1].copy()\n",
    "    order_items_local = liste_donnees[2].copy()\n",
    "    order_payments_local = liste_donnees[3].copy()\n",
    "    order_reviews_local = liste_donnees[4].copy()\n",
    "    orders_local = liste_donnees[5].copy()\n",
    "    products_local = liste_donnees[6].copy()\n",
    "    sellers_local = liste_donnees[7].copy()\n",
    "    translation_local = liste_donnees[8].copy()\n",
    "    \n",
    "    # Nombre de produits achetés par client\n",
    "    produits_par_client = pd.merge(customers_local, pd.merge(order_items_local, orders_local))\n",
    "    nb_produits = produits_par_client.groupby('customer_id')['product_id'].count()\n",
    "    nb_produits.rename('Nb_pdts', inplace=True)\n",
    "    \n",
    "    # 100 premières villes\n",
    "    first_cities = customers_local.groupby(['customer_city']).count()['customer_state'].sort_values(ascending=False).head(100).index.tolist()\n",
    "    index_cities = customers_local[~customers_local['customer_city'].isin(first_cities)].index\n",
    "    customers_local.loc[index_cities, 'customer_city'] = 'Other'\n",
    "    \n",
    "    # Catégorie la plus achetée\n",
    "    cat = pd.merge(produits_par_client, products_local).sort_values(['customer_id','product_category_name_english'], ascending=False).groupby(['customer_id','product_category_name_english']).head(1)[['customer_unique_id', 'product_category_name_english']]\n",
    "    cat.columns = ['customer_unique_id', 'Cat_la_plus_achetee']\n",
    "    cat.set_index('customer_unique_id', inplace=True)\n",
    "\n",
    "    # Montant moyen des achats\n",
    "    achats_moy = pd.merge(order_items_local, orders_local).groupby(['customer_id', 'order_id'])['price'].sum().groupby(['customer_id']).mean()\n",
    "    achats_moy.rename('Tot_moy_achats', inplace=True)\n",
    "\n",
    "    # Montant maximum des achats\n",
    "    achats_max = pd.merge(order_items_local, orders_local).groupby(['customer_id', 'order_id'])['price'].max().groupby(['customer_id']).max()\n",
    "    achats_max.rename('Mont_max_achats', inplace=True)\n",
    "    \n",
    "    # Nombre moyen de produits par commande\n",
    "    nb_moyen_prod = pd.merge(order_items_local, orders_local).groupby(['customer_id', 'order_id'])['price'].count().groupby(['customer_id']).mean()\n",
    "    nb_moyen_prod.rename('Nb_moy_pdts_par_com', inplace=True)\n",
    "    \n",
    "    # Délai moyen de livraison\n",
    "    delai_delivery = pd.merge(orders_local, order_items_local)\n",
    "    delai_delivery.set_index('customer_id', inplace=True)\n",
    "    delai_delivery = delai_delivery['order_delivered_customer_date'] - delai_delivery['order_purchase_timestamp']\n",
    "    delai_delivery.rename('Delai_Moy_Commande', inplace=True)\n",
    "    delai_delivery = delai_delivery.dt.days + 1\n",
    "    \n",
    "    # Heure du dernier achat\n",
    "    date_achat = pd.merge(order_items_local, orders_local)[['customer_id', 'order_purchase_timestamp']].groupby(['customer_id']).max()\n",
    "    date_achat['heure_achat'] = date_achat['order_purchase_timestamp'].apply(lambda x: x.hour)\n",
    "\n",
    "    # Jour de la semaine du dernier achat\n",
    "    date_achat['jour_achat'] = date_achat['order_purchase_timestamp'].apply(lambda x: x.weekday())\n",
    "\n",
    "    # Nombre de jours écoulés depuis le dernier achat\n",
    "    date_achat['delai_dernier_achat'] = date_achat['order_purchase_timestamp'].max() -  date_achat['order_purchase_timestamp']\n",
    "    date_achat['delai_dernier_achat'] = date_achat['delai_dernier_achat'].apply(lambda x: x.days)\n",
    "    \n",
    "    # Note moyenne des commentaires\n",
    "    note_moy = pd.merge(order_reviews_local, orders_local).groupby('customer_id')['review_score'].mean()\n",
    "    note_moy.rename('Note_Moy_Com', inplace=True)\n",
    "    \n",
    "    # Moyen de paiement le plus utilisé\n",
    "    paiement = pd.merge(orders_local, order_payments_local).sort_values(['customer_id','payment_type','payment_installments'], ascending=False).groupby(['customer_id', 'payment_type','payment_installments']).head(1)[['customer_id', 'payment_type','payment_installments']]\n",
    "    paiement.columns = ['customer_id', 'Moy_Paiment','Facilités']\n",
    "    paiement.set_index('customer_id', inplace=True)\n",
    "\n",
    "    customers_local.set_index('customer_id', inplace=True)\n",
    "    \n",
    "    # Merge des nouvelles features avec le dataframe customers_local\n",
    "    for df in [nb_produits, achats_moy, achats_max, delai_delivery, nb_moyen_prod, date_achat, note_moy, paiement]:\n",
    "        customers_local = pd.merge(left=customers_local, right=df, how='left', left_index=True, right_index=True)\n",
    "\n",
    "    customers_local.reset_index(inplace=True)\n",
    "    cat.reset_index(inplace=True)\n",
    "    \n",
    "    # Conversion des colonnes customer_id en type 'object'\n",
    "    customers_local['customer_id'] = customers_local['customer_id'].astype('object')\n",
    "    cat['customer_unique_id'] = cat['customer_unique_id'].astype('object')\n",
    "    \n",
    "    # Merge du dataframe customers_local avec la catégorie la plus achetée\n",
    "    customers_local = pd.merge(left=cat, right=customers_local, on='customer_unique_id', how='right')\n",
    "    customers_local.reset_index(inplace=True)\n",
    "    \n",
    "    print('### Cleaning NaN ###')\n",
    "    # Cleaning des NaN des features\n",
    "    customers_local['Cat_la_plus_achetee'].fillna('Unknown', inplace=True)\n",
    "    customers_local[['Nb_pdts', 'Tot_moy_achats', 'Mont_max_achats', 'Nb_moy_pdts_par_com']].dropna(how='all', axis=0, inplace=True)\n",
    "    customers_local['Delai_Moy_Commande'].fillna(-1, inplace=True)\n",
    "    customers_local.dropna(axis=0, inplace=True)\n",
    "\n",
    "    print('### Aggrégation des features sur id client unique ###')\n",
    "    # Agrégation des clients par identifiant unique\n",
    "    dict_agg = {\n",
    "        'Cat_la_plus_achetee': lambda x: x.mode()[0],\n",
    "        'customer_zip_code_prefix': lambda x: x.mode()[0],\n",
    "        'customer_city': lambda x: x.mode()[0],\n",
    "        'customer_state': lambda x: x.mode()[0],\n",
    "        'Nb_pdts': 'sum',\n",
    "        'Tot_moy_achats': 'mean',\n",
    "        'Mont_max_achats': 'max',\n",
    "        'Delai_Moy_Commande': 'mean',\n",
    "        'Nb_moy_pdts_par_com': 'mean',\n",
    "        'order_purchase_timestamp': 'max',\n",
    "        'heure_achat': lambda x: x.mode()[0],\n",
    "        'jour_achat': lambda x: x.mode()[0],\n",
    "        'delai_dernier_achat': 'min',\n",
    "        'Note_Moy_Com': 'mean',\n",
    "        'Moy_Paiment': lambda x: x.mode()[0],\n",
    "        'Facilités': 'mean',\n",
    "        'geolocation_lat': 'mean',\n",
    "        'geolocation_lng': 'mean',\n",
    "        ('price', 'home'): 'sum',\n",
    "        ('price', 'appliances'): 'sum',\n",
    "        ('price', 'construction'): 'sum',\n",
    "        ('price', 'office'): 'sum',\n",
    "        ('price', 'electronics'): 'sum',\n",
    "        ('price', 'arts'): 'sum',\n",
    "        ('price', 'fashion'): 'sum',\n",
    "        ('price', 'health_beauty'): 'sum',\n",
    "        ('price', 'sports_leisure'): 'sum',\n",
    "        ('price', 'garden_pets'): 'sum',\n",
    "        ('price', 'auto'): 'sum',\n",
    "        ('price', 'food_drinks'): 'sum',\n",
    "        ('price', 'other'): 'sum'\n",
    "    }\n",
    "\n",
    "    print('### Ajout des dépenses par catégories ###')\n",
    "    # Montant dépensé par catégorie de produits\n",
    "    table_cat = pd.merge(customers_local, pd.merge(pd.merge(products_local, translation_local), pd.merge(order_items_local, orders_local)))[['customer_unique_id', 'product_category_name_english', 'price']]\n",
    "    table_cat['product_category_name_english'] = table_cat['product_category_name_english'].map(dict_categories)\n",
    "    table_cat = pd.pivot_table(table_cat, index='customer_unique_id', columns='product_category_name_english', aggfunc=np.sum, fill_value=0).reset_index()\n",
    "    customers_local = pd.merge(customers_local, table_cat, left_on='customer_unique_id', right_on='customer_unique_id')\n",
    "    \n",
    "    # Merge des informations géographiques\n",
    "    geol = pd.merge(customers_local, geolocalisation_local, how='left', left_on='customer_zip_code_prefix', right_on='geolocation_zip_code_prefix').groupby('customer_unique_id').mean()[['geolocation_lat', 'geolocation_lng']].reset_index()\n",
    "    customers_local = pd.merge(customers_local, geol, left_on='customer_unique_id', right_on='customer_unique_id')\n",
    "    \n",
    "    # Agrégation finale par identifiant unique du client\n",
    "    customers_local = customers_local.groupby('customer_unique_id').agg(dict_agg)\n",
    "    customers_local['Cat_la_plus_achetee'] = customers_local['Cat_la_plus_achetee'].map(dict_categories)\n",
    "    \n",
    "    print('### Outliers ###')\n",
    "    # Nettoyage des outliers\n",
    "    print('taille du jeu de données pré-nettoyage outliers : ', customers_local.shape)\n",
    "    customers_local = clean_outliers(customers_local)\n",
    "    print('taille du jeu de données post-nettoyage outliers : ', customers_local.shape)\n",
    "    \n",
    "    print('### Log transformation ###')\n",
    "    # Transformation log\n",
    "    columns_log = ['Tot_moy_achats', 'Mont_max_achats']\n",
    "    customers_local[columns_log] = np.log(customers_local[columns_log])\n",
    "    return customers_local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explication étape par étape du code `apply_features` :\n",
    "\n",
    "1. **Création des nouvelles features :**\n",
    "   - Le code commence par créer plusieurs nouvelles features à partir des différentes tables de données (`customers_local`, `order_items_local`, `orders_local`, `order_reviews_local`, etc.).\n",
    "   - Par exemple, il calcule le nombre total de produits achetés par client (`Nb_pdts`), le montant moyen des achats (`Tot_moy_achats`), le montant maximum dépensé par client (`Mont_max_achats`), le délai moyen de livraison des commandes (`Delai_Moy_Commande`), le nombre moyen de produits par commande (`Nb_moy_pdts_par_com`), etc.\n",
    "   - Il récupère également l'heure, le jour de la semaine et le nombre de jours écoulés depuis le dernier achat (`heure_achat`, `jour_achat`, `delai_dernier_achat`).\n",
    "   - Il calcule la note moyenne des commentaires laissés par les clients (`Note_Moy_Com`) et identifie le moyen de paiement le plus utilisé par chaque client (`Moy_Paiment`).\n",
    "   - Il ajoute aussi des coordonnées géographiques (`geolocation_lat`, `geolocation_lng`) moyennes basées sur le code postal du client.\n",
    "\n",
    "2. **Nettoyage des NaN et Outliers :**\n",
    "   - Ensuite, le code effectue un nettoyage en supprimant les NaN dans les features nouvellement créées.\n",
    "   - Il supprime également les outliers (valeurs extrêmes) en utilisant deux fonctions `clean_outliers` : `delete_univariate_outliers` pour les valeurs extrêmes univariées et `delete_multivariate_outliers` pour les valeurs extrêmes multivariées.\n",
    "\n",
    "3. **Transformation log :**\n",
    "   - Certaines colonnes du dataframe sont ensuite transformées en prenant leur logarithme (`np.log`) pour mieux distribuer les valeurs et réduire l'effet des valeurs extrêmes.\n",
    "\n",
    "4. **Agrégation des données :**\n",
    "   - Une fois les features créées, nettoyées et transformées, le code procède à une agrégation des clients par identifiant unique (`customer_unique_id`).\n",
    "   - Il groupe les clients par cette clé unique et applique des fonctions d'agrégation pour consolider les valeurs des différentes features. Par exemple, pour certaines features, il prend la valeur moyenne (`mean`), pour d'autres, il somme les valeurs (`sum`), et pour d'autres encore, il prend le maximum (`max`).\n",
    "\n",
    "5. **Ajout des dépenses par catégories :**\n",
    "   - Le code calcule ensuite les montants dépensés par chaque client dans chaque catégorie de produits (`home`, `appliances`, `construction`, etc.).\n",
    "   - Il utilise une table pivot (`pd.pivot_table`) pour regrouper les montants dépensés par catégorie.\n",
    "\n",
    "6. **Merge des informations géographiques :**\n",
    "   - Les informations géographiques (latitude et longitude) sont ensuite ajoutées au dataframe principal en utilisant le code postal du client pour effectuer un merge avec la table `geolocalisation_local`.\n",
    "\n",
    "7. **Agrégation finale et nettoyage :**\n",
    "   - Enfin, le code effectue une dernière agrégation en utilisant les mêmes fonctions d'agrégation que précédemment pour consolider les informations géographiques.\n",
    "   - Il convertit également la catégorie la plus achetée (`Cat_la_plus_achetee`) en utilisant un dictionnaire de correspondance (`dict_categories`) pour des noms plus lisibles.\n",
    "   - Enfin, il effectue un dernier nettoyage en supprimant les éventuels NaN restants et les outliers, et retourne le dataframe nettoyé et agrégé.\n",
    "\n",
    "Le résultat final est un dataframe contenant toutes les informations agrégées par client, prêtes à être utilisées pour des analyses ultérieures ou pour construire un modèle de machine learning.\n",
    "\n",
    "###Création features###\n",
    "###Cleaning NaN###\n",
    "###Aggrégation des features sur id client unique###\n",
    "###Ajout des dépenses par catégories###\n",
    "###Outliers###\n",
    "taille du jeu de données pré-nettoyage outliers :  (93396, 31)\n",
    "taille du jeu de données post-nettoyage outliers :  (74705, 31)\n",
    "###Log transformation###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:06:55.907065Z",
     "start_time": "2020-01-16T07:01:18.328638Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# data_clients = apply_features(liste_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:06:56.027646Z",
     "start_time": "2020-01-16T07:06:55.908065Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# data_clients.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:07:48.663120Z",
     "start_time": "2020-01-16T07:07:48.648296Z"
    }
   },
   "outputs": [],
   "source": [
    "# data_clients.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## informations jeu de données initial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Historique**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:07:48.741365Z",
     "start_time": "2020-01-16T07:07:48.673264Z"
    }
   },
   "outputs": [],
   "source": [
    "orders['order_purchase_timestamp'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:07:48.773445Z",
     "start_time": "2020-01-16T07:07:48.745733Z"
    }
   },
   "outputs": [],
   "source": [
    "orders['order_purchase_timestamp'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a 2 années d'historique : de septembre 2016 à octobre 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nombre de clients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:07:48.844646Z",
     "start_time": "2020-01-16T07:07:48.777815Z"
    }
   },
   "outputs": [],
   "source": [
    "customers['customer_unique_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nombre de transactions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:07:48.901465Z",
     "start_time": "2020-01-16T07:07:48.848504Z"
    }
   },
   "outputs": [],
   "source": [
    "orders['order_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a presque autant de clients que de transactions => peu de clients ont fait plus d'une transaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nombre de vendeurs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:07:48.935956Z",
     "start_time": "2020-01-16T07:07:48.904306Z"
    }
   },
   "outputs": [],
   "source": [
    "sellers['seller_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sellers # ZIPCODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nombre de clients par vendeur**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:07:50.045333Z",
     "start_time": "2020-01-16T07:07:48.939642Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(order_items, orders).groupby(\n",
    "    ['seller_id','customer_id']).count().reset_index().groupby(\n",
    "    'seller_id').count()['customer_id'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## jeu de donnée préparé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T07:10:57.585533Z",
     "start_time": "2019-12-30T07:10:57.569912Z"
    }
   },
   "source": [
    "### Distribution des variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:07:59.637539Z",
     "start_time": "2020-01-16T07:07:50.046289Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "for column in data_clients.select_dtypes(['int32', 'float64']).columns:\n",
    "    f, axes = plt.subplots(1,2, figsize=(12,4))\n",
    "    titre = 'Distribution de ' + str(column)\n",
    "    plt.title(titre)\n",
    "    sns.distplot(data_clients[column], bins=30, ax=axes[0])\n",
    "    titre = 'Distribution de ' + str(column)\n",
    "    plt.title(titre)\n",
    "    sns.boxplot(data_clients[column], ax=axes[1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:08:06.258410Z",
     "start_time": "2020-01-16T07:07:59.645050Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "for column in data_clients.select_dtypes(['int32', 'float64']).columns:\n",
    "    if 'price' in column:\n",
    "        f, axes = plt.subplots(1,2, figsize=(12,4))\n",
    "        titre = 'Distribution de ' + str(column) +' sans valeur nulle'\n",
    "        plt.title(titre)\n",
    "        sns.distplot(data_clients[data_clients[column] != 0][column], bins=30, ax=axes[0])\n",
    "        titre = 'Distribution de ' + str(column) +'sans valeur nulle'\n",
    "        plt.title(titre)\n",
    "        sns.boxplot(data_clients[data_clients[column] != 0][column], ax=axes[1])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:08:06.594363Z",
     "start_time": "2020-01-16T07:08:06.264471Z"
    }
   },
   "outputs": [],
   "source": [
    "state = data_clients.groupby(['customer_state']).count().sort_values(\n",
    "    by='customer_city', ascending=False)['customer_city'].head(10)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title('Les 10 états avec le plus de clients')\n",
    "sns.barplot(x = state.values,\n",
    "           y = state.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:08:06.660963Z",
     "start_time": "2020-01-16T07:08:06.597946Z"
    }
   },
   "outputs": [],
   "source": [
    "data_clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:08:06.677801Z",
     "start_time": "2020-01-16T07:08:06.662697Z"
    }
   },
   "outputs": [],
   "source": [
    "data_clients['Cat_la_plus_achetee'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:08:06.719546Z",
     "start_time": "2020-01-16T07:08:06.681388Z"
    }
   },
   "outputs": [],
   "source": [
    "products['product_weight_g'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:08:07.011473Z",
     "start_time": "2020-01-16T07:08:06.722929Z"
    }
   },
   "outputs": [],
   "source": [
    "cities = data_clients.groupby(['customer_city']).count()[\n",
    "    'customer_state'].sort_values(ascending=False).head(10)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title('Les 10 villes avec le plus de clients')\n",
    "sns.barplot(x = cities.values,\n",
    "           y = cities.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:08:07.345277Z",
     "start_time": "2020-01-16T07:08:07.015520Z"
    }
   },
   "outputs": [],
   "source": [
    "categories = data_clients.groupby(['Cat_la_plus_achetee']).count()[\n",
    "    'customer_city'].sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.title('Répartition des catégories les plus achetées par les clients')\n",
    "sns.barplot(x = categories.values,\n",
    "           y = categories.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:08:07.578993Z",
     "start_time": "2020-01-16T07:08:07.348559Z"
    }
   },
   "outputs": [],
   "source": [
    "payment = data_clients.groupby(['Moy_Paiment']).count()[\n",
    "    'customer_city'].sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.title('Répartition des moyens de paiement plébiscités par les clients')\n",
    "sns.barplot(x = payment.values,\n",
    "           y = payment.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:08:07.979562Z",
     "start_time": "2020-01-16T07:08:07.581497Z"
    }
   },
   "outputs": [],
   "source": [
    "delai_achat = data_clients['order_purchase_timestamp'].max() - data_clients[\n",
    "    'order_purchase_timestamp']\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.title('Nombre de jours écoulés depuis la dernière commande')\n",
    "sns.distplot(delai_achat.dt.days, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions sur l'analyse des distributions des variables:**\n",
    "* très peu de clients ont fait plus d'un achat;\n",
    "* les distributions des montants d'achat par catégorie sont d'allure exponentielle\n",
    "* la variable price food drinks n'apporte pas d'information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrélations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:08:09.013605Z",
     "start_time": "2020-01-16T07:08:07.986734Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(data_clients.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilisation de la fonction avec votre DataFrame 'data_clients'\n",
    "show_correlogram(data_clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner les colonnes numériques\n",
    "numeric_columns = data_clients.select_dtypes(['int32', 'float64', 'int8', 'float32']).columns\n",
    "print(numeric_columns)\n",
    "pears_corr = data_clients[numeric_columns].corr()\n",
    "plot_heatmap(pears_corr, shape='tri', title=\"Pearson correlation\", figsize=(12, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit que certaines variables sont corrélées compte tenu du faible nombre de clients qui ont plusieurs transactions. C'est le cas de :\n",
    "* nb de produits achetés avec nombre moyen de produits par commande\n",
    "* montant total des achats avec montant moyen par commande\n",
    "\n",
    "Egalement, la feature ('price', 'food_drinks') ne contient pas d'information\n",
    "\n",
    "On va conserver une feature sur les 2 pour chaque corrélation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:08:09.050586Z",
     "start_time": "2020-01-16T07:08:09.017456Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_clients.drop(['Mont_max_achats','Nb_moy_pdts_par_com'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:08:09.130409Z",
     "start_time": "2020-01-16T07:08:09.056473Z"
    }
   },
   "outputs": [],
   "source": [
    "data_clients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export jeu de données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:08:09.198830Z",
     "start_time": "2020-01-16T07:08:09.136393Z"
    }
   },
   "outputs": [],
   "source": [
    "data_clients.select_dtypes(['object']).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:08:12.666999Z",
     "start_time": "2020-01-16T07:08:09.203565Z"
    }
   },
   "outputs": [],
   "source": [
    "data_clients.to_csv('data/data_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:08:12.767847Z",
     "start_time": "2020-01-16T07:08:12.670510Z"
    }
   },
   "outputs": [],
   "source": [
    "data_clients.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:08:13.150065Z",
     "start_time": "2020-01-16T07:08:12.776248Z"
    }
   },
   "outputs": [],
   "source": [
    "data_clients.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import folium\n",
    "\n",
    "# # Création d'une carte centrée sur la première ligne de vos données (par exemple)\n",
    "# latitude_centre = geolocalisation['geolocation_lat'].iloc[0]\n",
    "# longitude_centre = geolocalisation['geolocation_lng'].iloc[0]\n",
    "# carte = folium.Map(location=[latitude_centre, longitude_centre], zoom_start=10)\n",
    "\n",
    "# # Ajout de marqueurs pour chaque emplacement (latitude, longitude) dans vos données\n",
    "# for index, row in geolocalisation.iterrows():\n",
    "#     latitude = row['geolocation_lat']\n",
    "#     longitude = row['geolocation_lng']\n",
    "#     city = row['geolocation_city']\n",
    "#     popup_text = f\"Emplacement {index}: {city}\"  # Texte affiché dans le popup (avec la ville)\n",
    "#     folium.Marker([latitude, longitude], popup=popup_text).add_to(carte)\n",
    "\n",
    "# # Affichage de la carte\n",
    "# carte\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import de la librairie folium pour créer une carte interactive\n",
    "# import folium\n",
    "\n",
    "# # Création d'une carte centrée sur la position moyenne des points\n",
    "# m = folium.Map(location=[geolocalisation['geolocation_lat'].mean(), geolocalisation['geolocation_lng'].mean()], zoom_start=11)\n",
    "\n",
    "# # Boucle sur toutes les lignes du dataset\n",
    "# for i in range(0,len(geolocalisation)):\n",
    "#     # Ajout d'un cercle sur la carte à la position (latitude, longitude) de chaque ligne\n",
    "#     # avec un rayon de 100 mètres et un popup affichant l'adresse\n",
    "#     folium.Circle([geolocalisation.iloc[i]['geolocation_lat'],geolocalisation.iloc[i]['geolocation_lng']], popup=geolocalisation.iloc[i]['geolocation_city'], radius=100).add_to(m)\n",
    "\n",
    "# # Affichage de la carte interactive\n",
    "# m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ipyleaflet import Map, Marker\n",
    "\n",
    "# # Création d'une carte centrée sur la position moyenne des points\n",
    "# m = Map(center=[geolocalisation['geolocation_lat'].mean(), geolocalisation['geolocation_lng'].mean()], zoom=11)\n",
    "\n",
    "# # Boucle sur toutes les lignes du GeoDataFrame\n",
    "# for index, row in geolocalisation.iterrows():\n",
    "#     # Ajout d'un marqueur à chaque position géographique\n",
    "#     marker = Marker(location=[row['geolocation_lat'], row['geolocation_lng']], title=row['geolocation_city'])\n",
    "#     m.add_layer(marker)\n",
    "\n",
    "# # Affichage de la carte interactive\n",
    "# mf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import folium\n",
    "\n",
    "# # Création d'une nouvelle colonne 'coordinates' contenant les coordonnées géographiques sous forme de liste\n",
    "# geolocalisation['coordinates'] = geolocalisation.apply(lambda row: [row['geolocation_lat'], row['geolocation_lng']], axis=1)\n",
    "\n",
    "# # Création de la carte centrée sur la position moyenne des points\n",
    "# m = folium.Map(location=[geolocalisation['geolocation_lat'].mean(), geolocalisation['geolocation_lng'].mean()], zoom_start=11)\n",
    "\n",
    "# # Boucle sur toutes les lignes du dataset\n",
    "# for index, row in geolocalisation.iterrows():\n",
    "#     # Ajout d'un cercle sur la carte à la position (latitude, longitude) de chaque ligne\n",
    "#     # avec un rayon de 100 mètres et un popup affichant l'adresse\n",
    "#     folium.Circle(location=row['coordinates'], popup=row['geolocation_city'], radius=100).add_to(m)\n",
    "\n",
    "# # Affichage de la carte interactive\n",
    "# m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clients.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Afficher l'index actuel du DataFrame data_clients\n",
    "print(data_clients.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Réinitialiser l'index pour que 'customer_unique_id' redevienne une colonne du DataFrame\n",
    "data_clients_reset = data_clients.reset_index()\n",
    "\n",
    "# Calcul de la récence\n",
    "recency_df = data_clients_reset[['customer_unique_id', 'order_purchase_timestamp']]\n",
    "recency_df['order_purchase_timestamp'] = pd.to_datetime(recency_df['order_purchase_timestamp'])\n",
    "max_date = recency_df['order_purchase_timestamp'].max()\n",
    "recency_df['recency'] = (max_date - recency_df['order_purchase_timestamp']).dt.days\n",
    "\n",
    "# Supprimer la colonne 'order_purchase_timestamp' car elle n'est plus nécessaire\n",
    "recency_df.drop(columns=['order_purchase_timestamp'], inplace=True)\n",
    "\n",
    "# Calcul de la fréquence et de la valeur monétaire\n",
    "frequency_df = data_clients_reset.groupby('customer_unique_id').agg({'order_purchase_timestamp': 'count', 'Tot_moy_achats': 'sum'})\n",
    "# frequency_df.rename(columns={'order_purchase_timestamp': 'frequency', 'Mont_max_achats': 'monetary_value'}, inplace=True)\n",
    "frequency_df.rename(columns={'order_purchase_timestamp': 'frequency', 'Tot_moy_achats': 'monetary_value'}, inplace=True)\n",
    "\n",
    "# Concaténer les trois mesures : récence, fréquence et valeur monétaire\n",
    "rfm_df = pd.concat([recency_df.set_index('customer_unique_id'), frequency_df], axis=1)\n",
    "\n",
    "# Affichage des 5 premières lignes du DataFrame rfm_df\n",
    "print(rfm_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Création d'un diagramme en nuage de points (scatter plot)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(rfm_df['recency'], rfm_df['frequency'], c=rfm_df['monetary_value'], cmap='viridis', alpha=0.8)\n",
    "plt.colorbar(label='Valeur monétaire')\n",
    "plt.xlabel('Récence (jours)')\n",
    "plt.ylabel('Fréquence')\n",
    "plt.title('Segmentation RFM')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Création d'un graphique en 3D\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Ajout des points sur le graphique en 3D\n",
    "ax.scatter(rfm_df['recency'], rfm_df['frequency'], rfm_df['monetary_value'], c=rfm_df['monetary_value'], cmap='viridis', alpha=0.8)\n",
    "\n",
    "# Configuration des axes\n",
    "ax.set_xlabel('Récence (jours)')\n",
    "ax.set_ylabel('Fréquence')\n",
    "ax.set_zlabel('Valeur monétaire')\n",
    "ax.set_title('Segmentation RFM')\n",
    "\n",
    "# Ajout de la barre de couleur pour représenter la valeur monétaire\n",
    "cbar = plt.colorbar(ax.scatter([], [], [], c=[], cmap='viridis'), ax=ax)\n",
    "cbar.set_label('Valeur monétaire')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Utilisation de Folium :\n",
    "\n",
    "# import folium\n",
    "\n",
    "# # Création d'une nouvelle colonne 'coordinates' contenant les coordonnées géographiques sous forme de liste\n",
    "# geolocalisation['coordinates'] = geolocalisation.apply(lambda row: [row['geolocation_lat'], row['geolocation_lng']], axis=1)\n",
    "\n",
    "# # Création de la carte centrée sur la position moyenne des points\n",
    "# m = folium.Map(location=[geolocalisation['geolocation_lat'].mean(), geolocalisation['geolocation_lng'].mean()], zoom_start=11)\n",
    "\n",
    "# # Boucle sur toutes les lignes du dataset\n",
    "# for index, row in geolocalisation.iterrows():\n",
    "#     # Ajout d'un cercle sur la carte à la position (latitude, longitude) de chaque ligne\n",
    "#     # avec un rayon de 100 mètres et un popup affichant l'adresse\n",
    "#     folium.Circle(location=row['coordinates'], popup=row['geolocation_city'], radius=100).add_to(m)\n",
    "\n",
    "# # Affichage de la carte interactive\n",
    "# m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Utilisation de Geopandas (pour les données géospatiales) :\n",
    "\n",
    "# import geopandas as gpd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Création du GeoDataFrame à partir du DataFrame geolocalisation\n",
    "# gdf = gpd.GeoDataFrame(\n",
    "#     geolocalisation, \n",
    "#     geometry=gpd.points_from_xy(geolocalisation['geolocation_lng'], geolocalisation['geolocation_lat'])\n",
    "# )\n",
    "\n",
    "# # Création de la carte centrée sur la position moyenne des points\n",
    "# ax = gdf.plot(figsize=(10, 10), alpha=0.5, markersize=10)\n",
    "\n",
    "# # Affichage de la carte\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Utilisation de Geopandas (pour les données géospatiales) :\n",
    "\n",
    "# import geopandas as gpd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Création du GeoDataFrame à partir du DataFrame geolocalisation\n",
    "# gdf = gpd.GeoDataFrame(\n",
    "#     data_clients, \n",
    "#     geometry=gpd.points_from_xy(data_clients['geolocation_lng'], data_clients['geolocation_lat'])\n",
    "# )\n",
    "\n",
    "# # Création de la carte centrée sur la position moyenne des points\n",
    "# ax = gdf.plot(figsize=(10, 10), alpha=0.5, markersize=10)\n",
    "\n",
    "# # Affichage de la carte\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import folium\n",
    "\n",
    "# # Création de la carte centrée sur la position moyenne des points\n",
    "# m = folium.Map(location=[geolocalisation['geolocation_lat'].mean(), geolocalisation['geolocation_lng'].mean()], zoom_start=11)\n",
    "\n",
    "# # Boucle sur toutes les lignes du GeoDataFrame\n",
    "# for index, row in gdf.iterrows():\n",
    "#     # Ajout d'un marqueur à chaque position géographique\n",
    "#     folium.Marker([row['geolocation_lat'], row['geolocation_lng']], popup=row['geolocation_city']).add_to(m)\n",
    "\n",
    "# # Affichage de la carte interactive\n",
    "# m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Utilisation de Plotly (Mapbox) :\n",
    "# import plotly.graph_objects as go\n",
    "\n",
    "# # Création de la carte centrée sur la position moyenne des points\n",
    "# m = go.Figure(go.Scattermapbox(\n",
    "#     lat=geolocalisation['geolocation_lat'],\n",
    "#     lon=geolocalisation['geolocation_lng'],\n",
    "#     mode='markers',\n",
    "#     marker=go.scattermapbox.Marker(size=10),\n",
    "#     text=geolocalisation['geolocation_city'],\n",
    "# ))\n",
    "\n",
    "# m.update_layout(\n",
    "#     mapbox_style='open-street-map',\n",
    "#     mapbox_center={'lat': geolocalisation['geolocation_lat'].mean(), 'lon': geolocalisation['geolocation_lng'].mean()},\n",
    "#     mapbox_zoom=11,\n",
    "# )\n",
    "\n",
    "# # Affichage de la carte interactive\n",
    "# m.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer la bibliothèque Plotly pour créer une carte interactive\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Création de la carte centrée sur la position moyenne des points\n",
    "m = go.Figure(go.Scattermapbox(\n",
    "    lat=data_clients['geolocation_lat'],      # Coordonnées de latitude des points\n",
    "    lon=data_clients['geolocation_lng'],      # Coordonnées de longitude des points\n",
    "    mode='markers',                           # Mode d'affichage des marqueurs (ici, des points)\n",
    "    marker=go.scattermapbox.Marker(size=10),  # Taille des marqueurs (points) sur la carte\n",
    "    text=data_clients['customer_city'],       # Texte affiché lorsqu'on survole un point (ville du client)\n",
    "))\n",
    "\n",
    "# Mise à jour du style de la carte avec une carte de fond \"open-street-map\"\n",
    "# Définir une variable pour stocker le niveau de zoom souhaité\n",
    "zoom_level = 2\n",
    "\n",
    "m.update_layout(\n",
    "    mapbox_style='open-street-map',\n",
    "    \n",
    "    # Centrage de la carte sur la position moyenne des points (latitude et longitude)\n",
    "    mapbox_center={'lat': data_clients['geolocation_lat'].mean(), 'lon': data_clients['geolocation_lng'].mean()},\n",
    "    \n",
    "    # Utiliser la variable 'zoom_level' pour définir le niveau de zoom initial de la carte\n",
    "    mapbox_zoom=zoom_level,\n",
    ")\n",
    "\n",
    "# Affichage de la carte interactive\n",
    "m.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import folium\n",
    "\n",
    "# # Création de la carte centrée sur la position moyenne des points\n",
    "# m = folium.Map(location=[data_clients['geolocation_lat'].mean(), data_clients['geolocation_lng'].mean()], zoom_start=11)\n",
    "\n",
    "# # Boucle sur toutes les lignes du GeoDataFrame\n",
    "# for index, row in gdf.iterrows():\n",
    "#     # Ajout d'un marqueur à chaque position géographique\n",
    "#     folium.Marker([row['geolocation_lat'], row['geolocation_lng']], popup=row['customer_city']).add_to(m)\n",
    "\n",
    "# # Affichage de la carte interactive\n",
    "# m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1)\n",
    "# sns.scatterplot(data_clients['geolocation_lat'], data_clients['geolocation_lng'],\n",
    "#                 s=5, hue = np.log(data_clients['customer_city']),\n",
    "#                 alpha=0.7, marker='o', ec=None, palette='viridis')\n",
    "# ax.set_axis_off()\n",
    "# ax.set_aspect('equal', adjustable='box')\n",
    "# ax.legend(loc=3)\n",
    "# fig.set_size_inches(10,10)\n",
    "# fig.savefig('customers_loc.png', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "\n",
    "# Charger les données depuis les fichiers CSV\n",
    "customers = pd.read_csv('data/olist_customers_dataset.csv')\n",
    "order_items = pd.read_csv('data/olist_order_items_dataset.csv')\n",
    "order_payments = pd.read_csv('data/olist_order_payments_dataset.csv')\n",
    "orders = pd.read_csv('data/olist_orders_dataset.csv')\n",
    "\n",
    "# Fusionner les DataFrames pour avoir toutes les informations nécessaires en un seul DataFrame\n",
    "df = orders.merge(customers, on='customer_id')\n",
    "df = df.merge(order_items, on='order_id')\n",
    "df = df.merge(order_payments, on='order_id')\n",
    "\n",
    "# Convertir la colonne 'order_purchase_timestamp' en type datetime\n",
    "df['order_purchase_timestamp'] = pd.to_datetime(df['order_purchase_timestamp'])\n",
    "\n",
    "# Calculer la date de la dernière commande (récence)\n",
    "current_date = df['order_purchase_timestamp'].max()\n",
    "df['recency'] = (current_date - df['order_purchase_timestamp']).dt.days\n",
    "\n",
    "# Calculer la fréquence des commandes par client\n",
    "frequency_df = df.groupby('customer_unique_id')['order_id'].nunique().reset_index()\n",
    "frequency_df.rename(columns={'order_id': 'frequency'}, inplace=True)\n",
    "df = df.merge(frequency_df, on='customer_unique_id', how='left')\n",
    "\n",
    "# Calculer le montant total dépensé par client\n",
    "df['total_spent'] = df['payment_value']\n",
    "\n",
    "# Maintenant que nous avons les colonnes Récence, Fréquence et Montant, nous pouvons appliquer la méthode RFM\n",
    "\n",
    "# Fonction pour attribuer un score à chaque dimension RFM en fonction des quartiles\n",
    "def assign_rfm_score(x, quartiles):\n",
    "    if x <= quartiles[0]:  # Quartile le plus bas\n",
    "        return 1\n",
    "    elif x <= quartiles[1]:  # Deuxième quartile\n",
    "        return 2\n",
    "    elif x <= quartiles[2]:  # Troisième quartile\n",
    "        return 3\n",
    "    else:  # Quartile le plus élevé\n",
    "        return 4\n",
    "\n",
    "# Calculer les quartiles pour chaque dimension RFM\n",
    "recency_quartiles = df['recency'].quantile([0.25, 0.5, 0.75]).values\n",
    "frequency_quartiles = df['frequency'].quantile([0.25, 0.5, 0.75]).values\n",
    "monetary_quartiles = df['total_spent'].quantile([0.25, 0.5, 0.75]).values\n",
    "\n",
    "# Assigner les scores RFM à chaque client\n",
    "df['R'] = df['recency'].apply(assign_rfm_score, args=(recency_quartiles,))\n",
    "df['F'] = df['frequency'].apply(assign_rfm_score, args=(frequency_quartiles,))\n",
    "df['M'] = df['total_spent'].apply(assign_rfm_score, args=(monetary_quartiles,))\n",
    "\n",
    "# Calculer le score RFM total en concaténant les scores R, F et M\n",
    "df['RFM_Score'] = df['R'].astype(str) + df['F'].astype(str) + df['M'].astype(str)\n",
    "\n",
    "# Maintenant, vous avez le score RFM pour chaque client dans la colonne 'RFM_Score'\n",
    "# Vous pouvez utiliser ce score pour segmenter vos clients en différentes catégories RFM.\n",
    "\n",
    "# Par exemple, les clients ayant un score RFM de '111' sont considérés comme les plus précieux,\n",
    "# tandis que ceux ayant un score de '444' sont considérés comme les moins précieux.\n",
    "\n",
    "# Vous pouvez également attribuer des étiquettes (comme 'Platinum', 'Gold', 'Silver', 'Bronze', etc.)\n",
    "# en fonction des scores RFM pour identifier les groupes de clients avec différentes caractéristiques de comportement d'achat.\n",
    "\n",
    "# Pour analyser davantage les segments RFM, vous pouvez utiliser des graphiques, des tableaux croisés dynamiques, etc.\n",
    "\n",
    "# Exemple de segmentation RFM simple :\n",
    "def segment_rfm(df):\n",
    "    if df['RFM_Score'] == '111':\n",
    "        return 'Premium'\n",
    "    elif df['RFM_Score'] == '444':\n",
    "        return 'Low Value'\n",
    "    else:\n",
    "        return 'Standard'\n",
    "\n",
    "# Appliquer la segmentation RFM au DataFrame\n",
    "df['RFM_Segment'] = df.apply(segment_rfm, axis=1)\n",
    "\n",
    "# Vous pouvez également enregistrer les résultats dans un nouveau fichier CSV, si nécessaire\n",
    "df.to_csv('resultats_rfm.csv', index=False)\n",
    "\n",
    "# Créer un graphique pour visualiser la répartition des segments RFM\n",
    "plt.figure(figsize=(10, 6))\n",
    "df['RFM_Segment'].value_counts().plot(kind='bar', color='skyblue')\n",
    "plt.title('Répartition des segments RFM')\n",
    "plt.xlabel('Segments RFM')\n",
    "plt.ylabel('Nombre de clients')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Création d'un diagramme en nuage de points (scatter plot)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df['recency'], df['frequency'], c=df['total_spent'], cmap='viridis', alpha=0.8)\n",
    "plt.colorbar(label='Valeur monétaire')\n",
    "plt.xlabel('Récence (jours)')\n",
    "plt.ylabel('Fréquence')\n",
    "plt.title('Segmentation RFM')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# # Assigner les scores RFM à chaque client\n",
    "# df['R'] = df['recency'].apply(assign_rfm_score, args=(recency_quartiles,))\n",
    "# df['F'] = df['frequency'].apply(assign_rfm_score, args=(frequency_quartiles,))\n",
    "# df['M'] = df['total_spent'].apply(assign_rfm_score, args=(monetary_quartiles,))\n",
    "# Création d'un graphique en 3D\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Ajout des points sur le graphique en 3D\n",
    "ax.scatter(df['recency'], df['frequency'], df['total_spent'], c=df['total_spent'], cmap='viridis', alpha=0.8)\n",
    "\n",
    "# Configuration des axes\n",
    "ax.set_xlabel('Récence (jours)')\n",
    "ax.set_ylabel('Fréquence')\n",
    "ax.set_zlabel('Valeur monétaire')\n",
    "ax.set_title('Segmentation RFM')\n",
    "\n",
    "# Ajout de la barre de couleur pour représenter la valeur monétaire\n",
    "cbar = plt.colorbar(ax.scatter([], [], [], c=[], cmap='viridis'), ax=ax)\n",
    "cbar.set_label('Valeur monétaire')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import plotly.graph_objects as go\n",
    "\n",
    "# # Charger les données\n",
    "# geolocation = pd.read_csv('data/olist_geolocation_dataset.csv')\n",
    "# customer = pd.read_csv('data/olist_customers_dataset.csv')\n",
    "# sellers = pd.read_csv('data/olist_sellers_dataset.csv')\n",
    "\n",
    "# # Merge des informations géographiques\n",
    "# geol = pd.merge(customer, geolocation, how='left', left_on='customer_zip_code_prefix', right_on='geolocation_zip_code_prefix').groupby('customer_unique_id').mean()[['geolocation_lat', 'geolocation_lng']].reset_index()\n",
    "# customer_merged = pd.merge(customer, geol, left_on='customer_unique_id', right_on='customer_unique_id')\n",
    "\n",
    "    \n",
    "# # # Fusionner les données de géolocalisation avec les données des clients\n",
    "# # customer_merged = customer.merge(geolocation, on='geolocation_zip_code_prefix', how='left')\n",
    "\n",
    "# # Fusionner les données de géolocalisation avec les données des vendeurs\n",
    "# sellers_merged = sellers.merge(geolocation, left_on='seller_zip_code_prefix', right_on='geolocation_zip_code_prefix', how='left')\n",
    "\n",
    "# # Créer une carte interactive pour afficher les vendeurs et les clients par pays\n",
    "# world_map = go.Figure()\n",
    "\n",
    "# # Ajouter les marqueurs (vendeurs) à la carte\n",
    "# world_map.add_trace(\n",
    "#     go.Scattergeo(\n",
    "#         lat=sellers_merged['geolocation_lat'],   # Utiliser les latitudes des vendeurs comme emplacements pour les marqueurs\n",
    "#         lon=sellers_merged['geolocation_lng'],   # Utiliser les longitudes des vendeurs comme emplacements pour les marqueurs\n",
    "#         mode='markers',\n",
    "#         marker=dict(size=10, color='blue'),\n",
    "#         text=sellers_merged['seller_city'],\n",
    "#         hoverinfo='text'\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# # Ajouter les marqueurs (clients) à la carte\n",
    "# world_map.add_trace(\n",
    "#     go.Scattergeo(\n",
    "#         lat=customer_merged['geolocation_lat'],   # Utiliser les latitudes des clients comme emplacements pour les marqueurs\n",
    "#         lon=customer_merged['geolocation_lng'],   # Utiliser les longitudes des clients comme emplacements pour les marqueurs\n",
    "#         mode='markers',\n",
    "#         marker=dict(size=5, color='red'),\n",
    "#         text=customer_merged['customer_city'],\n",
    "#         hoverinfo='text'\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# # Mise en forme de la carte\n",
    "# world_map.update_layout(\n",
    "#     title_text='Carte des vendeurs et des clients',\n",
    "#     title_x=0.5,\n",
    "#     geo=dict(\n",
    "#         scope='world',  # Afficher le monde entier sur la carte\n",
    "#         showland=True,\n",
    "#         landcolor='rgb(217, 217, 217)',\n",
    "#         showcountries=True,\n",
    "#         countrycolor='rgb(255, 255, 255)',\n",
    "#         showocean=True,\n",
    "#         oceancolor='rgb(0, 204, 204)',\n",
    "#         showcoastlines=True,\n",
    "#         coastlinecolor='rgb(255, 255, 255)',\n",
    "#         showframe=False,\n",
    "#         showrivers=False,\n",
    "#         showlakes=True,\n",
    "#         lakecolor='rgb(0, 255, 255)'\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# # Affichage de la carte interactive\n",
    "# world_map.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carte Client et vendeur (menu déroulant) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Charger les données\n",
    "geolocation = pd.read_csv('data/olist_geolocation_dataset.csv')\n",
    "customer = pd.read_csv('data/olist_customers_dataset.csv')\n",
    "sellers = pd.read_csv('data/olist_sellers_dataset.csv')\n",
    "\n",
    "# Merge des informations géographiques\n",
    "geol = pd.merge(customer, geolocation, how='left', left_on='customer_zip_code_prefix', right_on='geolocation_zip_code_prefix').groupby('customer_unique_id').mean()[['geolocation_lat', 'geolocation_lng']].reset_index()\n",
    "customer_merged = pd.merge(customer, geol, left_on='customer_unique_id', right_on='customer_unique_id')\n",
    "\n",
    "# Fusionner les données de géolocalisation avec les données des vendeurs\n",
    "sellers_merged = sellers.merge(geolocation, left_on='seller_zip_code_prefix', right_on='geolocation_zip_code_prefix', how='left')\n",
    "\n",
    "# Créer une carte interactive pour afficher les vendeurs et les clients par pays\n",
    "world_map = go.Figure()\n",
    "\n",
    "# Ajouter les marqueurs (vendeurs) à la carte\n",
    "vendeurs_trace = go.Scattergeo(\n",
    "    lat=sellers_merged['geolocation_lat'],   # Utiliser les latitudes des vendeurs comme emplacements pour les marqueurs\n",
    "    lon=sellers_merged['geolocation_lng'],   # Utiliser les longitudes des vendeurs comme emplacements pour les marqueurs\n",
    "    mode='markers',\n",
    "    marker=dict(size=10, color='blue'),\n",
    "    text=sellers_merged['seller_city'],\n",
    "    hoverinfo='text',\n",
    "    name='Vendeurs'\n",
    ")\n",
    "\n",
    "# Ajouter les marqueurs (clients) à la carte\n",
    "clients_trace = go.Scattergeo(\n",
    "    lat=customer_merged['geolocation_lat'],   # Utiliser les latitudes des clients comme emplacements pour les marqueurs\n",
    "    lon=customer_merged['geolocation_lng'],   # Utiliser les longitudes des clients comme emplacements pour les marqueurs\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color='red'),\n",
    "    text=customer_merged['customer_city'],\n",
    "    hoverinfo='text',\n",
    "    name='Clients'\n",
    ")\n",
    "\n",
    "# Ajouter les traces au layout de la carte\n",
    "world_map.add_trace(vendeurs_trace)\n",
    "world_map.add_trace(clients_trace)\n",
    "\n",
    "# Mise en forme du menu déroulant\n",
    "world_map.update_layout(\n",
    "    title_text='Carte des vendeurs et des clients',\n",
    "    title_x=0.5,\n",
    "    geo=dict(\n",
    "        scope='world',  # Afficher le monde entier sur la carte\n",
    "        showland=True,\n",
    "        landcolor='rgb(217, 217, 217)',\n",
    "        showcountries=True,\n",
    "        countrycolor='rgb(255, 255, 255)',\n",
    "        showocean=True,\n",
    "        oceancolor='rgb(0, 204, 204)',\n",
    "        showcoastlines=True,\n",
    "        coastlinecolor='rgb(255, 255, 255)',\n",
    "        showframe=False,\n",
    "        showrivers=False,\n",
    "        showlakes=True,\n",
    "        lakecolor='rgb(0, 255, 255)'\n",
    "    ),\n",
    "    # Ajouter un menu déroulant pour afficher tous, vendeurs ou clients\n",
    "    updatemenus=[\n",
    "        {\n",
    "            'buttons': [\n",
    "                {\n",
    "                    'args': [{'visible': [True, True]}, {'title': 'Carte des vendeurs et des clients'}],\n",
    "                    'label': 'Tous',\n",
    "                    'method': 'update'\n",
    "                },\n",
    "                {\n",
    "                    'args': [{'visible': [True, False]}, {'title': 'Carte des vendeurs'}],\n",
    "                    'label': 'Vendeurs',\n",
    "                    'method': 'update'\n",
    "                },\n",
    "                {\n",
    "                    'args': [{'visible': [False, True]}, {'title': 'Carte des clients'}],\n",
    "                    'label': 'Clients',\n",
    "                    'method': 'update'\n",
    "                }\n",
    "            ],\n",
    "            'direction': 'down',\n",
    "            'showactive': True,\n",
    "            'x': 0.1,\n",
    "            'xanchor': 'left',\n",
    "            'y': 1.1,\n",
    "            'yanchor': 'top'\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Affichage de la carte interactive\n",
    "world_map.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les données\n",
    "customers = pd.read_csv('data/olist_customers_dataset.csv')\n",
    "geolocation = pd.read_csv('data/olist_geolocation_dataset.csv')\n",
    "order_items = pd.read_csv('data/olist_order_items_dataset.csv')\n",
    "order_payments = pd.read_csv('data/olist_order_payments_dataset.csv')\n",
    "order_reviews = pd.read_csv('data/olist_order_reviews_dataset.csv')\n",
    "orders = pd.read_csv('data/olist_orders_dataset.csv')\n",
    "products = pd.read_csv('data/olist_products_dataset.csv')\n",
    "sellers = pd.read_csv('data/olist_sellers_dataset.csv')\n",
    "translation = pd.read_csv('data/product_category_name_translation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer une connexion à la base de données DuckDB\n",
    "con = ddb.connect()\n",
    "\n",
    "# Requête SQL pour récupérer les localisations des vendeurs sans valeurs nulles ou NaN\n",
    "query = \"\"\"\n",
    "    SELECT\n",
    "        c.seller_id AS id,\n",
    "        c.seller_city AS city,\n",
    "        c.seller_state AS state,\n",
    "        AVG(g.geolocation_lat) AS lat,\n",
    "        AVG(g.geolocation_lng) AS lng\n",
    "    FROM sellers AS c\n",
    "    LEFT JOIN geolocation AS g ON c.seller_zip_code_prefix = g.geolocation_zip_code_prefix\n",
    "    WHERE g.geolocation_lat IS NOT NULL AND g.geolocation_lng IS NOT NULL\n",
    "    GROUP BY c.seller_id, c.seller_city, c.seller_state\n",
    "\"\"\"\n",
    "\n",
    "# Exécuter la requête\n",
    "locations_df = con.execute(query).fetchdf()\n",
    "locations_df.sample(5)\n",
    "# # Créer une carte\n",
    "# m = folium.Map(location=[locations_df['lat'].mean(), locations_df['lng'].mean()], zoom_start=4)\n",
    "\n",
    "# # Ajouter des marqueurs à la carte\n",
    "# marker_cluster = MarkerCluster().add_to(m)\n",
    "# for idx, row in locations_df.iterrows():\n",
    "#     folium.Marker(\n",
    "#         location=[row['lat'], row['lng']],\n",
    "#         popup=row['city'] + ', ' + row['state']\n",
    "#     ).add_to(marker_cluster)\n",
    "\n",
    "# # Afficher le menu déroulant\n",
    "# folium.LayerControl().add_to(m)\n",
    "\n",
    "# # Afficher la carte interactive\n",
    "# m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du GeoDataFrame à partir du DataFrame geolocalisation\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    locations_df, \n",
    "    geometry=gpd.points_from_xy(locations_df['lng'], locations_df['lat'])\n",
    ")\n",
    "\n",
    "# Création de la carte centrée sur la position moyenne des points\n",
    "ax = gdf.plot(figsize=(10, 10), alpha=0.5, markersize=10)\n",
    "\n",
    "# Affichage de la carte\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer la bibliothèque Plotly pour créer une carte interactive\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Création de la carte centrée sur la position moyenne des points\n",
    "m = go.Figure(go.Scattermapbox(\n",
    "    lat=locations_df['lat'],      # Coordonnées de latitude des points\n",
    "    lon=locations_df['lng'],      # Coordonnées de longitude des points\n",
    "    mode='markers',                           # Mode d'affichage des marqueurs (ici, des points)\n",
    "    marker=go.scattermapbox.Marker(size=10),  # Taille des marqueurs (points) sur la carte\n",
    "    text=locations_df['city'],       # Texte affiché lorsqu'on survole un point (ville du client)\n",
    "))\n",
    "\n",
    "# Mise à jour du style de la carte avec une carte de fond \"open-street-map\"\n",
    "# Définir une variable pour stocker le niveau de zoom souhaité\n",
    "zoom_level = 2\n",
    "\n",
    "m.update_layout(\n",
    "    mapbox_style='open-street-map',\n",
    "    \n",
    "    # Centrage de la carte sur la position moyenne des points (latitude et longitude)\n",
    "    mapbox_center={'lat': locations_df['lat'].mean(), 'lon': locations_df['lng'].mean()},\n",
    "    \n",
    "    # Utiliser la variable 'zoom_level' pour définir le niveau de zoom initial de la carte\n",
    "    mapbox_zoom=zoom_level,\n",
    ")\n",
    "\n",
    "# Affichage de la carte interactive\n",
    "m.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de la carte\n",
    "m = folium.Map(location=[locations_df['lat'].mean(), locations_df['lng'].mean()], zoom_start=4)\n",
    "\n",
    "# Ajout des clusters de marqueurs\n",
    "marker_cluster = MarkerCluster().add_to(m)\n",
    "for idx, row in locations_df.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[row['lat'], row['lng']],\n",
    "        popup=row['city'] + ', ' + row['state']\n",
    "    ).add_to(marker_cluster)\n",
    "\n",
    "# Afficher le menu déroulant\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "# Afficher la carte interactive\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer une connexion à la base de données DuckDB\n",
    "con = ddb.connect()\n",
    "\n",
    "# # Requête SQL pour récupérer les localisations uniques des clients sans valeurs nulles ou NaN\n",
    "# query = \"\"\"\n",
    "#     SELECT DISTINCT\n",
    "#         c.customer_id AS id,\n",
    "#         c.customer_city AS city,\n",
    "#         c.customer_state AS state,\n",
    "#         g.geolocation_lat AS lat,\n",
    "#         g.geolocation_lng AS lng\n",
    "#     FROM customers AS c\n",
    "#     LEFT JOIN geolocation AS g ON c.customer_zip_code_prefix = g.geolocation_zip_code_prefix\n",
    "#     WHERE g.geolocation_lat IS NOT NULL AND g.geolocation_lng IS NOT NULL\n",
    "# \"\"\"\n",
    "# Requête SQL pour récupérer les localisations uniques des clients sans valeurs nulles ou NaN\n",
    "query = \"\"\"\n",
    "    SELECT\n",
    "        c.customer_id AS id,\n",
    "        c.customer_city AS city,\n",
    "        c.customer_state AS state,\n",
    "        AVG(g.geolocation_lat) AS lat,\n",
    "        AVG(g.geolocation_lng) AS lng\n",
    "    FROM customers AS c\n",
    "    LEFT JOIN geolocation AS g ON c.customer_zip_code_prefix = g.geolocation_zip_code_prefix\n",
    "    WHERE g.geolocation_lat IS NOT NULL AND g.geolocation_lng IS NOT NULL\n",
    "    GROUP BY c.customer_id, c.customer_city, c.customer_state\n",
    "\"\"\"\n",
    "\n",
    "# Exécuter la requête\n",
    "locations_df = con.execute(query).fetchdf()\n",
    "locations_df.sample(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du GeoDataFrame à partir du DataFrame geolocalisation\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    locations_df, \n",
    "    geometry=gpd.points_from_xy(locations_df['lng'], locations_df['lat'])\n",
    ")\n",
    "\n",
    "# Création de la carte centrée sur la position moyenne des points\n",
    "ax = gdf.plot(figsize=(10, 10), alpha=0.5, markersize=10)\n",
    "\n",
    "# Affichage de la carte #\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import folium\n",
    "\n",
    "# # Création de la carte centrée sur la position moyenne des points\n",
    "# m = folium.Map(location=[locations_df['lat'].mean(), locations_df['lng'].mean()], zoom_start=10)\n",
    "\n",
    "# # Ajout des marqueurs pour chaque point\n",
    "# for idx, row in locations_df.iterrows():\n",
    "#     folium.Marker([row['lat'], row['lng']]).add_to(m)\n",
    "\n",
    "# # Affichage de la carte\n",
    "# m.save('map.html')  # Sauvegarde de la carte dans un fichier HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer la bibliothèque Plotly pour créer une carte interactive\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Création de la carte centrée sur la position moyenne des points\n",
    "m = go.Figure(go.Scattermapbox(\n",
    "    lat=locations_df['lat'],      # Coordonnées de latitude des points\n",
    "    lon=locations_df['lng'],      # Coordonnées de longitude des points\n",
    "    mode='markers',                           # Mode d'affichage des marqueurs (ici, des points)\n",
    "    marker=go.scattermapbox.Marker(size=10),  # Taille des marqueurs (points) sur la carte\n",
    "    text=locations_df['city'],       # Texte affiché lorsqu'on survole un point (ville du client)\n",
    "))\n",
    "\n",
    "# Mise à jour du style de la carte avec une carte de fond \"open-street-map\"\n",
    "# Définir une variable pour stocker le niveau de zoom souhaité\n",
    "zoom_level = 2\n",
    "\n",
    "m.update_layout(\n",
    "    mapbox_style='open-street-map',\n",
    "    \n",
    "    # Centrage de la carte sur la position moyenne des points (latitude et longitude)\n",
    "    mapbox_center={'lat': locations_df['lat'].mean(), 'lon': locations_df['lng'].mean()},\n",
    "    \n",
    "    # Utiliser la variable 'zoom_level' pour définir le niveau de zoom initial de la carte\n",
    "    mapbox_zoom=zoom_level,\n",
    ")\n",
    "\n",
    "# Affichage de la carte interactive\n",
    "m.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de la carte\n",
    "m = folium.Map(location=[locations_df['lat'].mean(), locations_df['lng'].mean()], zoom_start=4)\n",
    "\n",
    "# Ajout des clusters de marqueurs\n",
    "marker_cluster = MarkerCluster().add_to(m)\n",
    "for idx, row in locations_df.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[row['lat'], row['lng']],\n",
    "        popup=row['city'] + ', ' + row['state']\n",
    "    ).add_to(marker_cluster)\n",
    "\n",
    "# Afficher le menu déroulant\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "# Afficher la carte interactive\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution des commandes par mois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders[\"period\"] = orders[[\"order_purchase_timestamp\"]].applymap(lambda o: str(dt.datetime.strptime(\n",
    "    o, '%Y-%m-%d %H:%M:%S').year * 100 + dt.datetime.strptime(o, '%Y-%m-%d %H:%M:%S').month))\n",
    "sorted_orders = orders.sort_values(by=[\"period\"])\n",
    "\n",
    "fig = plt.figure(figsize=[24, 8])\n",
    "sns.displot(sorted_orders[\"period\"])\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "rfm_df = ddb.query(\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "        customer_unique_id, \n",
    "        MAX(order_purchase_timestamp) as last_order,\n",
    "        COUNT(DISTINCT o.order_id) as frequency,\n",
    "        SUM(payment_value) as monetary_value\n",
    "    FROM customers as c\n",
    "    JOIN orders as o on c.customer_id = o.customer_id\n",
    "    JOIN order_payments as p on o.order_id = p.order_id\n",
    "    WHERE order_purchase_timestamp <= '2018-08-31 23:59:59'\n",
    "    AND   order_purchase_timestamp >= '2017-09-01 00:00:00'\n",
    "    GROUP BY customer_unique_id\n",
    "    HAVING SUM(payment_value) <= 5500\n",
    "    \"\"\"\n",
    ").to_df()\n",
    "\n",
    "rfm_df[\"recency\"] = rfm_df[[\"last_order\"]].applymap(lambda o: (dt.datetime.strptime('2018-08-31 23:59:59', '%Y-%m-%d %H:%M:%S') - dt.datetime.strptime(o, '%Y-%m-%d %H:%M:%S')).days)\n",
    "rfm_df[\"mean_monetary_value\"] = rfm_df.monetary_value / rfm_df.frequency\n",
    "\n",
    "# Calculer la date de la dernière commande (récence)\n",
    "# current_date = df['order_purchase_timestamp'].max()\n",
    "# df['recency'] = (current_date - df['order_purchase_timestamp']).dt.days\n",
    "\n",
    "rfm_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Création d'un graphique en 3D\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Ajout des points sur le graphique en 3D\n",
    "# ax.scatter(rfm_df['recency'], rfm_df['frequency'], rfm_df['monetary_value'], c=rfm_df['monetary_value'], cmap='viridis', alpha=0.8)\n",
    "ax.scatter(rfm_df['recency'], rfm_df['frequency'], rfm_df['mean_monetary_value'], c=rfm_df['mean_monetary_value'], cmap='viridis', alpha=0.8)\n",
    "\n",
    "# Configuration des axes\n",
    "ax.set_xlabel('Récence (jours)')\n",
    "ax.set_ylabel('Fréquence')\n",
    "ax.set_zlabel('Valeur monétaire')\n",
    "ax.set_title('Segmentation RFM')\n",
    "\n",
    "# Ajout de la barre de couleur pour représenter la valeur monétaire\n",
    "cbar = plt.colorbar(ax.scatter([], [], [], c=[], cmap='viridis'), ax=ax)\n",
    "cbar.set_label('Valeur monétaire')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=rfm_df.recency, y=rfm_df.monetary_value)\n",
    "plt.show()\n",
    "sns.scatterplot(x=rfm_df.recency, y=rfm_df.frequency)\n",
    "plt.show()\n",
    "sns.scatterplot(x=rfm_df.frequency, y=rfm_df.recency)\n",
    "plt.show()\n",
    "sns.scatterplot(x=rfm_df.frequency, y=rfm_df.monetary_value)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=rfm_df.recency, y=rfm_df.mean_monetary_value)\n",
    "plt.show()\n",
    "sns.scatterplot(x=rfm_df.recency, y=rfm_df.frequency)\n",
    "plt.show()\n",
    "sns.scatterplot(x=rfm_df.frequency, y=rfm_df.recency)\n",
    "plt.show()\n",
    "sns.scatterplot(x=rfm_df.frequency, y=rfm_df.mean_monetary_value)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| v Commandes - récence -> | Low | Middle | Top |\n",
    "| --- | --- | --- | --- |\n",
    "| Low |  |  |  |\n",
    "| Middle |  | Main group | A relancer |\n",
    "| Top |  |  | Gold Customer |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enregistrer le DataFrame dans un fichier CSV\n",
    "rfm_df.to_csv('data/data_cleaned_rfm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "352px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
