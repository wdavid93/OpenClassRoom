{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21d5cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "def evaluate_models(data):\n",
    "    # Charger votre DataFrame (assurez-vous d'avoir une colonne 'text' contenant le texte et une colonne 'category' pour les étiquettes)\n",
    "    df = pd.read_csv(data)\n",
    "\n",
    "    # Séparer les données en ensembles d'entraînement et de test\n",
    "    X = df['text']\n",
    "    y = df['category']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Encoder les étiquettes\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "    y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "    # Tokenization et padding des séquences\n",
    "    max_words = 1000\n",
    "    tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "    X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "    max_sequence_length = 100  # Vous pouvez ajuster la longueur maximale de la séquence selon vos besoins\n",
    "    X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
    "    X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
    "\n",
    "    # Créer le modèle LSTM\n",
    "    embedding_dim = 100\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length))\n",
    "    model.add(LSTM(100))  # Vous pouvez ajuster le nombre de neurones LSTM selon vos besoins\n",
    "    model.add(Dense(len(label_encoder.classes_), activation='softmax'))  # Couche de sortie avec le nombre de classes\n",
    "\n",
    "    # Compiler le modèle\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # Entraîner le modèle avec EarlyStopping pour éviter le surapprentissage\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    model.fit(X_train_padded, y_train_encoded, epochs=20, batch_size=64, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "    # Évaluer le modèle\n",
    "    test_loss, test_accuracy = model.evaluate(X_test_padded, y_test_encoded)\n",
    "    print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "    # Faites des prédictions\n",
    "    predictions = model.predict(X_test_padded)\n",
    "\n",
    "    # Vous pouvez décoder les prédictions si nécessaire\n",
    "    decoded_predictions = label_encoder.inverse_transform(predictions.argmax(axis=1))\n",
    "\n",
    "    # Calcul des métriques\n",
    "    accuracy = accuracy_score(y_test_encoded, decoded_predictions)\n",
    "    precision = precision_score(y_test_encoded, decoded_predictions, average='weighted')\n",
    "    recall = recall_score(y_test_encoded, decoded_predictions, average='weighted')\n",
    "    f1 = f1_score(y_test_encoded, decoded_predictions, average='weighted')\n",
    "\n",
    "    # Présenter une synthèse comparative des différents modèles\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# Exemple d'utilisation de la fonction\n",
    "evaluate_models('votre_dataframe.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801a798f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from nlpaug.util.file.download import DownloadUtil\n",
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "def data_augmentation(data):\n",
    "    # Charger votre DataFrame (assurez-vous d'avoir une colonne 'text' contenant le texte et une colonne 'category' pour les étiquettes)\n",
    "    df = pd.read_csv(data)\n",
    "\n",
    "    # Séparer les données en ensembles d'entraînement et de test\n",
    "    X = df['text']\n",
    "    y = df['category']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Data augmentation avec NLPAug\n",
    "    aug = naw.SynonymAug(aug_src='wordnet')\n",
    "    augmented_X_train = [aug.augment(text) for text in X_train]\n",
    "\n",
    "    # Tokenization et padding des séquences\n",
    "    max_words = 1000\n",
    "    tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    X_train_sequences = tokenizer.texts_to_sequences(augmented_X_train)\n",
    "    X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "    max_sequence_length = 100  # Vous pouvez ajuster la longueur maximale de la séquence selon vos besoins\n",
    "    X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
    "    X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
    "\n",
    "    # Créer le modèle LSTM\n",
    "    embedding_dim = 100\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length))\n",
    "    model.add(LSTM(100))  # Vous pouvez ajuster le nombre de neurones LSTM selon vos besoins\n",
    "    model.add(Dense(len(label_encoder.classes_), activation='softmax'))  # Couche de sortie avec le nombre de classes\n",
    "\n",
    "    # Compiler le modèle\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # Entraîner le modèle avec EarlyStopping pour éviter le surapprentissage\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    model.fit(X_train_padded, y_train_encoded, epochs=20, batch_size=64, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "    # Évaluer le modèle\n",
    "    test_loss, test_accuracy = model.evaluate(X_test_padded, y_test_encoded)\n",
    "    print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "    # Présenter une synthèse comparative des améliorations de performance\n",
    "    print(\"Comparaison avant et après l'augmentation de données :\")\n",
    "    print(f\"Accuracy avant augmentation : {accuracy_before_augmentation}\")\n",
    "    print(f\"Accuracy après augmentation : {test_accuracy}\")\n",
    "\n",
    "# Exemple d'utilisation de la fonction\n",
    "data_augmentation('votre_dataframe.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af329536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.applications.vgg16 import VGG16, decode_predictions\n",
    "from keras.preprocessing.image import load_img, img_to_array, preprocess_input\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras import optimizers\n",
    "\n",
    "def load_data(data_dir):\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for class_folder in os.listdir(data_dir):\n",
    "        class_folder_path = os.path.join(data_dir, class_folder)\n",
    "\n",
    "        if os.path.isdir(class_folder_path):\n",
    "            for image_file in os.listdir(class_folder_path):\n",
    "                if image_file.endswith('.jpg'):\n",
    "                    image_path = os.path.join(class_folder_path, image_file)\n",
    "                    image = cv2.imread(image_path)\n",
    "                    if image is not None:\n",
    "                        images.append(image)\n",
    "                        labels.append(class_folder)\n",
    "\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "def preprocess_data(images, labels, test_size=0.2, random_state=42):\n",
    "    images = images / 255.0\n",
    "    train_images, test_images, train_labels, test_labels = train_test_split(\n",
    "        images, labels, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    return train_images, test_images, train_labels, test_labels\n",
    "\n",
    "def create_custom_model(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (3, 3), input_shape=input_shape, padding='same', activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "def load_vgg16_model(num_classes):\n",
    "    model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    \n",
    "    # Add custom classification layers\n",
    "    x = model.output\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(4096, activation='relu')(x)\n",
    "    x = Dense(4096, activation='relu')(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    # Create a new model\n",
    "    custom_model = Model(inputs=model.input, outputs=predictions)\n",
    "    \n",
    "    return custom_model\n",
    "\n",
    "def fine_tune_total(model):\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = True\n",
    "\n",
    "def feature_extraction(model):\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "def fine_tune_partial(model, num_layers_to_freeze):\n",
    "    for layer in model.layers[:num_layers_to_freeze]:\n",
    "        layer.trainable = False\n",
    "\n",
    "def train_model(model, train_images, train_labels, epochs=10, batch_size=32):\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(train_images, train_labels, epochs=epochs, batch_size=batch_size, verbose=2)\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, test_images, test_labels):\n",
    "    test_loss, test_accuracy = model.evaluate(test_images, test_labels, verbose=2)\n",
    "    print(f\"Test accuracy: {test_accuracy*100:.2f}%\")\n",
    "\n",
    "def predict(model, image_path):\n",
    "    img = load_img(image_path, target_size=(224, 224))\n",
    "    img = img_to_array(img)\n",
    "    img = img.reshape((1, img.shape[0], img.shape[1], img.shape[2]))\n",
    "    img = preprocess_input(img)\n",
    "    \n",
    "    prediction = model.predict(img)\n",
    "    return decode_predictions(prediction, top=3)[0]\n",
    "\n",
    "def main():\n",
    "    data_dir = 'path_to_data_directory'\n",
    "    images, labels = load_data(data_dir)\n",
    "    train_images, test_images, train_labels, test_labels = preprocess_data(images, labels)\n",
    "    \n",
    "    # Custom Model\n",
    "    custom_model = create_custom_model(train_images.shape[1:], len(np.unique(labels)))\n",
    "    fine_tune_total(custom_model)\n",
    "    custom_model = train_model(custom_model, train_images, train_labels)\n",
    "    evaluate_model(custom_model, test_images, test_labels)\n",
    "    \n",
    "    # VGG-16 Transfer Learning\n",
    "    vgg16_model = load_vgg16_model(len(np.unique(labels)))\n",
    "    fine_tune_partial(vgg16_model, 5)\n",
    "    vgg16_model = train_model(vgg16_model, train_images, train_labels)\n",
    "    evaluate_model(vgg16_model, test_images, test_labels)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
